01/25/2022 06:28:13 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=10, config_name='graphcodebert-base', dev_filename='dataset/evosuit/Evosuit_test_lang3_ArrayUtils.methods,dataset/evosuit/Evosuit_test_lang3_ArrayUtils.tests', do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=64, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='saved_models/dec_6_evosuit_graphcodebert_512_320/methods-tests//checkpoint-best-bleu/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=512, max_steps=-1, max_target_length=320, model_name_or_path='saved_models/dec_6_evosuit_graphcodebert_512_320/methods-tests//checkpoint-best-bleu/pytorch_model.bin', model_type='roberta', no_cuda=False, num_train_epochs=3, output_dir='saved_models/dec_6_evosuit_graphcodebert_512_320/methods-tests/', seed=42, source_lang='methods', test_filename='dataset/evosuit/Evosuit_test_lang3_ArrayUtils.methods,dataset/evosuit/Evosuit_test_lang3_ArrayUtils.tests', tokenizer_name='graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
Some weights of the model checkpoint at saved_models/dec_6_evosuit_graphcodebert_512_320/methods-tests//checkpoint-best-bleu/pytorch_model.bin were not used when initializing RobertaModel: ['encoder.encoder.layer.11.attention.self.query.bias', 'decoder.layers.0.multihead_attn.in_proj_bias', 'encoder.encoder.layer.1.output.LayerNorm.weight', 'encoder.encoder.layer.11.intermediate.dense.bias', 'decoder.layers.1.multihead_attn.out_proj.bias', 'decoder.layers.2.norm2.weight', 'encoder.embeddings.token_type_embeddings.weight', 'encoder.encoder.layer.0.output.dense.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'decoder.layers.0.self_attn.in_proj_weight', 'decoder.layers.5.linear1.weight', 'decoder.layers.5.norm1.weight', 'encoder.encoder.layer.9.attention.self.key.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'decoder.layers.3.linear2.weight', 'decoder.layers.1.linear1.bias', 'decoder.layers.1.multihead_attn.out_proj.weight', 'encoder.encoder.layer.5.intermediate.dense.bias', 'encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'decoder.layers.2.linear1.weight', 'decoder.layers.3.self_attn.in_proj_bias', 'encoder.encoder.layer.9.intermediate.dense.weight', 'encoder.encoder.layer.7.intermediate.dense.bias', 'decoder.layers.2.norm1.weight', 'encoder.encoder.layer.3.attention.output.dense.weight', 'decoder.layers.1.linear2.bias', 'decoder.layers.5.norm3.weight', 'encoder.encoder.layer.4.attention.self.value.weight', 'encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.encoder.layer.5.output.LayerNorm.bias', 'encoder.encoder.layer.7.attention.output.dense.weight', 'decoder.layers.0.multihead_attn.out_proj.bias', 'encoder.encoder.layer.5.attention.output.dense.bias', 'encoder.encoder.layer.6.intermediate.dense.weight', 'decoder.layers.3.multihead_attn.in_proj_bias', 'encoder.encoder.layer.6.attention.self.value.weight', 'encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'decoder.layers.5.norm2.weight', 'encoder.encoder.layer.8.intermediate.dense.bias', 'encoder.encoder.layer.4.output.LayerNorm.bias', 'decoder.layers.4.linear1.bias', 'encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'decoder.layers.3.norm1.weight', 'encoder.encoder.layer.1.attention.output.dense.weight', 'encoder.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.encoder.layer.6.attention.output.dense.weight', 'encoder.encoder.layer.10.attention.output.dense.bias', 'decoder.layers.0.linear2.bias', 'encoder.encoder.layer.2.attention.output.dense.weight', 'encoder.encoder.layer.9.attention.output.dense.weight', 'encoder.encoder.layer.0.output.dense.bias', 'encoder.encoder.layer.2.attention.self.key.weight', 'decoder.layers.2.norm3.weight', 'encoder.encoder.layer.11.attention.output.dense.bias', 'encoder.encoder.layer.8.output.LayerNorm.bias', 'encoder.encoder.layer.2.intermediate.dense.bias', 'encoder.encoder.layer.4.attention.self.key.bias', 'encoder.encoder.layer.5.attention.self.value.bias', 'decoder.layers.3.norm2.weight', 'decoder.layers.0.norm3.weight', 'encoder.encoder.layer.3.attention.self.key.weight', 'encoder.encoder.layer.3.attention.output.dense.bias', 'decoder.layers.2.self_attn.out_proj.weight', 'encoder.encoder.layer.5.attention.self.query.weight', 'decoder.layers.2.linear2.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.output.LayerNorm.weight', 'encoder.encoder.layer.10.output.LayerNorm.weight', 'decoder.layers.1.norm2.weight', 'decoder.layers.2.self_attn.in_proj_weight', 'encoder.encoder.layer.1.output.dense.weight', 'decoder.layers.0.self_attn.in_proj_bias', 'encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.encoder.layer.11.attention.self.value.weight', 'encoder.encoder.layer.4.attention.self.key.weight', 'bias', 'encoder.encoder.layer.7.attention.self.query.bias', 'encoder.encoder.layer.0.intermediate.dense.bias', 'encoder.encoder.layer.8.intermediate.dense.weight', 'encoder.encoder.layer.11.attention.self.key.bias', 'decoder.layers.3.self_attn.in_proj_weight', 'encoder.encoder.layer.7.attention.output.dense.bias', 'decoder.layers.3.linear1.weight', 'encoder.embeddings.LayerNorm.weight', 'encoder.encoder.layer.0.attention.output.dense.bias', 'encoder.encoder.layer.0.attention.output.dense.weight', 'encoder.encoder.layer.4.intermediate.dense.weight', 'encoder.encoder.layer.11.attention.output.dense.weight', 'encoder.encoder.layer.7.output.dense.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.encoder.layer.0.intermediate.dense.weight', 'encoder.encoder.layer.10.attention.self.key.weight', 'encoder.encoder.layer.7.attention.self.query.weight', 'encoder.encoder.layer.8.attention.output.dense.bias', 'decoder.layers.3.linear2.bias', 'encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'decoder.layers.3.norm2.bias', 'decoder.layers.2.norm3.bias', 'encoder.encoder.layer.8.attention.output.dense.weight', 'decoder.layers.5.norm2.bias', 'encoder.encoder.layer.11.output.dense.weight', 'decoder.layers.3.linear1.bias', 'encoder.encoder.layer.3.intermediate.dense.weight', 'encoder.encoder.layer.9.output.dense.bias', 'encoder.encoder.layer.3.attention.self.key.bias', 'decoder.layers.1.multihead_attn.in_proj_weight', 'encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.encoder.layer.5.output.dense.weight', 'encoder.encoder.layer.2.attention.output.dense.bias', 'encoder.encoder.layer.5.attention.self.query.bias', 'encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'decoder.layers.0.norm1.bias', 'encoder.encoder.layer.1.output.dense.bias', 'encoder.encoder.layer.2.output.LayerNorm.weight', 'encoder.encoder.layer.6.output.dense.weight', 'encoder.encoder.layer.9.output.LayerNorm.bias', 'decoder.layers.0.multihead_attn.in_proj_weight', 'encoder.encoder.layer.3.output.LayerNorm.bias', 'decoder.layers.1.linear1.weight', 'encoder.encoder.layer.1.output.LayerNorm.bias', 'encoder.encoder.layer.3.output.dense.bias', 'decoder.layers.3.multihead_attn.out_proj.bias', 'encoder.encoder.layer.2.output.LayerNorm.bias', 'encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.self.key.weight', 'encoder.encoder.layer.10.intermediate.dense.bias', 'encoder.encoder.layer.6.attention.output.dense.bias', 'encoder.encoder.layer.6.attention.self.query.bias', 'encoder.encoder.layer.9.attention.self.value.bias', 'encoder.encoder.layer.7.attention.self.value.bias', 'encoder.encoder.layer.0.attention.self.value.bias', 'encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'decoder.layers.1.norm1.weight', 'encoder.encoder.layer.0.attention.self.key.bias', 'decoder.layers.5.linear1.bias', 'decoder.layers.4.norm3.weight', 'decoder.layers.0.multihead_attn.out_proj.weight', 'decoder.layers.0.linear1.weight', 'encoder.encoder.layer.1.intermediate.dense.weight', 'decoder.layers.1.multihead_attn.in_proj_bias', 'decoder.layers.1.self_attn.in_proj_weight', 'decoder.layers.1.norm3.weight', 'decoder.layers.3.multihead_attn.in_proj_weight', 'decoder.layers.0.norm2.bias', 'decoder.layers.5.norm3.bias', 'encoder.encoder.layer.4.attention.self.value.bias', 'decoder.layers.4.multihead_attn.in_proj_weight', 'encoder.embeddings.LayerNorm.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.4.norm1.bias', 'decoder.layers.5.linear2.bias', 'encoder.encoder.layer.6.output.LayerNorm.bias', 'encoder.encoder.layer.5.attention.self.value.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'encoder.encoder.layer.0.attention.self.value.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'encoder.encoder.layer.10.output.dense.bias', 'encoder.pooler.dense.bias', 'encoder.encoder.layer.5.attention.self.key.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.encoder.layer.9.output.LayerNorm.weight', 'encoder.encoder.layer.6.intermediate.dense.bias', 'encoder.encoder.layer.0.output.LayerNorm.bias', 'encoder.encoder.layer.3.intermediate.dense.bias', 'encoder.encoder.layer.7.intermediate.dense.weight', 'encoder.encoder.layer.4.output.dense.bias', 'decoder.layers.2.linear1.bias', 'encoder.encoder.layer.1.attention.self.value.weight', 'encoder.encoder.layer.1.attention.self.query.bias', 'encoder.encoder.layer.10.attention.self.key.bias', 'decoder.layers.2.self_attn.out_proj.bias', 'encoder.encoder.layer.2.attention.self.value.bias', 'encoder.encoder.layer.7.output.LayerNorm.bias', 'encoder.encoder.layer.11.intermediate.dense.weight', 'encoder.encoder.layer.2.attention.self.query.bias', 'decoder.layers.3.norm3.weight', 'encoder.encoder.layer.7.output.LayerNorm.weight', 'encoder.encoder.layer.6.attention.self.key.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.1.self_attn.in_proj_bias', 'decoder.layers.5.multihead_attn.out_proj.weight', 'encoder.encoder.layer.1.attention.output.dense.bias', 'decoder.layers.1.linear2.weight', 'dense.weight', 'decoder.layers.5.linear2.weight', 'encoder.encoder.layer.6.output.LayerNorm.weight', 'decoder.layers.4.self_attn.in_proj_weight', 'encoder.encoder.layer.5.intermediate.dense.weight', 'decoder.layers.1.norm2.bias', 'encoder.encoder.layer.8.output.dense.weight', 'encoder.encoder.layer.1.intermediate.dense.bias', 'decoder.layers.2.multihead_attn.out_proj.bias', 'encoder.encoder.layer.11.output.LayerNorm.weight', 'encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.encoder.layer.10.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.self.key.weight', 'encoder.encoder.layer.3.attention.self.value.weight', 'encoder.encoder.layer.10.attention.self.value.weight', 'encoder.encoder.layer.6.attention.self.key.weight', 'encoder.encoder.layer.11.attention.self.query.weight', 'encoder.encoder.layer.2.output.dense.weight', 'decoder.layers.2.norm1.bias', 'encoder.encoder.layer.5.attention.output.dense.weight', 'decoder.layers.2.multihead_attn.out_proj.weight', 'decoder.layers.1.self_attn.out_proj.weight', 'encoder.embeddings.position_ids', 'decoder.layers.4.linear2.bias', 'encoder.encoder.layer.8.attention.self.value.bias', 'decoder.layers.5.multihead_attn.in_proj_bias', 'decoder.layers.3.norm3.bias', 'decoder.layers.0.linear1.bias', 'encoder.encoder.layer.8.attention.self.key.bias', 'encoder.encoder.layer.9.attention.self.query.weight', 'encoder.encoder.layer.8.attention.self.query.bias', 'encoder.encoder.layer.0.attention.self.query.weight', 'encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'decoder.layers.4.multihead_attn.out_proj.weight', 'encoder.encoder.layer.10.attention.output.dense.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'encoder.encoder.layer.3.output.LayerNorm.weight', 'encoder.encoder.layer.9.output.dense.weight', 'encoder.encoder.layer.10.intermediate.dense.weight', 'decoder.layers.0.norm3.bias', 'decoder.layers.3.norm1.bias', 'decoder.layers.5.self_attn.in_proj_bias', 'encoder.encoder.layer.8.attention.self.value.weight', 'encoder.encoder.layer.10.attention.self.query.weight', 'encoder.encoder.layer.9.intermediate.dense.bias', 'encoder.encoder.layer.7.output.dense.bias', 'decoder.layers.5.norm1.bias', 'decoder.layers.0.linear2.weight', 'decoder.layers.1.norm3.bias', 'encoder.encoder.layer.4.attention.self.query.weight', 'decoder.layers.2.self_attn.in_proj_bias', 'decoder.layers.0.norm2.weight', 'decoder.layers.4.linear1.weight', 'encoder.encoder.layer.6.output.dense.bias', 'encoder.encoder.layer.8.output.dense.bias', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.4.multihead_attn.in_proj_bias', 'encoder.encoder.layer.7.attention.self.value.weight', 'encoder.encoder.layer.9.attention.self.query.bias', 'encoder.encoder.layer.9.attention.output.dense.bias', 'encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'decoder.layers.4.norm3.bias', 'dense.bias', 'encoder.encoder.layer.0.attention.self.query.bias', 'decoder.layers.0.norm1.weight', 'encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.encoder.layer.11.output.dense.bias', 'encoder.encoder.layer.11.attention.self.value.bias', 'encoder.encoder.layer.5.output.LayerNorm.weight', 'decoder.layers.1.norm1.bias', 'encoder.encoder.layer.4.intermediate.dense.bias', 'decoder.layers.5.multihead_attn.in_proj_weight', 'decoder.layers.5.multihead_attn.out_proj.bias', 'encoder.encoder.layer.2.output.dense.bias', 'encoder.encoder.layer.10.attention.self.query.bias', 'encoder.encoder.layer.2.attention.self.key.bias', 'encoder.encoder.layer.6.attention.self.value.bias', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.4.norm2.weight', 'encoder.embeddings.word_embeddings.weight', 'decoder.layers.2.multihead_attn.in_proj_weight', 'encoder.encoder.layer.6.attention.self.query.weight', 'encoder.encoder.layer.2.attention.self.query.weight', 'encoder.encoder.layer.10.output.dense.weight', 'encoder.encoder.layer.7.attention.self.key.weight', 'encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'decoder.layers.4.linear2.weight', 'encoder.encoder.layer.5.attention.self.key.bias', 'encoder.encoder.layer.3.output.dense.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'encoder.encoder.layer.3.attention.self.query.bias', 'encoder.encoder.layer.3.attention.self.value.bias', 'decoder.layers.2.norm2.bias', 'encoder.encoder.layer.10.attention.self.value.bias', 'lm_head.weight', 'encoder.encoder.layer.11.attention.self.key.weight', 'encoder.encoder.layer.11.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.self.value.bias', 'encoder.encoder.layer.3.attention.self.query.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.5.self_attn.in_proj_weight', 'encoder.pooler.dense.weight', 'encoder.encoder.layer.4.attention.output.dense.weight', 'encoder.encoder.layer.4.output.dense.weight', 'decoder.layers.2.multihead_attn.in_proj_bias', 'encoder.encoder.layer.1.attention.self.key.bias', 'decoder.layers.4.norm2.bias', 'encoder.encoder.layer.9.attention.self.value.weight', 'encoder.encoder.layer.7.attention.self.key.bias', 'decoder.layers.3.multihead_attn.out_proj.weight', 'encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.encoder.layer.8.attention.self.query.weight', 'encoder.encoder.layer.2.intermediate.dense.weight', 'decoder.layers.4.self_attn.in_proj_bias', 'decoder.layers.4.norm1.weight', 'decoder.layers.4.multihead_attn.out_proj.bias', 'decoder.layers.2.linear2.bias', 'encoder.encoder.layer.4.attention.output.dense.bias', 'encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.attention.self.query.bias', 'encoder.encoder.layer.8.attention.self.key.weight', 'encoder.encoder.layer.8.output.LayerNorm.weight', 'encoder.encoder.layer.0.output.LayerNorm.weight', 'encoder.encoder.layer.5.output.dense.bias', 'encoder.encoder.layer.9.attention.self.key.bias', 'encoder.encoder.layer.1.attention.self.query.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at saved_models/dec_6_evosuit_graphcodebert_512_320/methods-tests//checkpoint-best-bleu/pytorch_model.bin and are newly initialized: ['encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.6.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'pooler.dense.bias', 'encoder.layer.3.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/25/2022 06:28:19 - INFO - __main__ -   reload model from saved_models/dec_6_evosuit_graphcodebert_512_320/methods-tests//checkpoint-best-bleu/pytorch_model.bin
01/25/2022 06:28:24 - INFO - __main__ -   Test file: dataset/evosuit/Evosuit_test_lang3_ArrayUtils.methods,dataset/evosuit/Evosuit_test_lang3_ArrayUtils.tests
  0%|          | 0/682 [00:00<?, ?it/s]  1%|▏         | 9/682 [00:00<00:07, 85.35it/s]  4%|▎         | 25/682 [00:00<00:05, 127.80it/s]  6%|▌         | 42/682 [00:00<00:04, 141.82it/s]  8%|▊         | 57/682 [00:00<00:04, 128.00it/s] 11%|█         | 73/682 [00:00<00:04, 136.05it/s] 13%|█▎        | 90/682 [00:00<00:04, 145.45it/s] 16%|█▌        | 107/682 [00:00<00:03, 148.06it/s] 18%|█▊        | 122/682 [00:00<00:04, 127.12it/s] 20%|█▉        | 136/682 [00:01<00:04, 116.73it/s] 22%|██▏       | 149/682 [00:01<00:04, 107.44it/s] 26%|██▌       | 177/682 [00:01<00:03, 147.67it/s] 28%|██▊       | 193/682 [00:01<00:03, 149.16it/s] 32%|███▏      | 216/682 [00:01<00:02, 168.45it/s] 34%|███▍      | 234/682 [00:01<00:02, 150.05it/s] 37%|███▋      | 250/682 [00:01<00:03, 131.37it/s] 39%|███▊      | 264/682 [00:01<00:03, 126.63it/s] 41%|████      | 278/682 [00:02<00:03, 116.27it/s] 43%|████▎     | 291/682 [00:02<00:03, 109.82it/s] 46%|████▌     | 315/682 [00:02<00:02, 138.96it/s] 48%|████▊     | 330/682 [00:02<00:02, 125.43it/s] 50%|█████     | 344/682 [00:02<00:02, 122.82it/s] 52%|█████▏    | 358/682 [00:02<00:02, 125.69it/s] 55%|█████▍    | 372/682 [00:02<00:02, 127.36it/s] 57%|█████▋    | 386/682 [00:03<00:02, 112.29it/s] 59%|█████▊    | 400/682 [00:03<00:02, 117.95it/s] 61%|██████    | 417/682 [00:03<00:02, 131.22it/s] 63%|██████▎   | 433/682 [00:03<00:01, 138.43it/s] 66%|██████▋   | 452/682 [00:03<00:01, 150.33it/s] 69%|██████▉   | 469/682 [00:03<00:01, 152.90it/s] 71%|███████▏  | 486/682 [00:03<00:01, 156.54it/s] 74%|███████▍  | 503/682 [00:03<00:01, 157.88it/s] 76%|███████▋  | 521/682 [00:03<00:00, 162.18it/s] 79%|███████▉  | 538/682 [00:03<00:00, 149.98it/s] 83%|████████▎ | 563/682 [00:04<00:00, 176.97it/s] 86%|████████▌ | 587/682 [00:04<00:00, 194.43it/s] 89%|████████▉ | 607/682 [00:04<00:00, 162.12it/s] 92%|█████████▏| 625/682 [00:04<00:00, 141.05it/s] 94%|█████████▍| 641/682 [00:04<00:00, 136.87it/s] 96%|█████████▌| 656/682 [00:04<00:00, 128.33it/s] 98%|█████████▊| 670/682 [00:04<00:00, 127.65it/s]100%|██████████| 682/682 [00:05<00:00, 136.27it/s]
  0%|          | 0/11 [00:00<?, ?it/s]/project/6025819/sepehr8/test_generation/GraphCodeBERT_TestGen/model.py:175: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  prevK = bestScoresId // numWords
  9%|▉         | 1/11 [03:19<33:16, 199.61s/it] 18%|█▊        | 2/11 [06:41<30:07, 200.83s/it] 27%|██▋       | 3/11 [09:55<26:23, 197.98s/it] 36%|███▋      | 4/11 [13:07<22:47, 195.33s/it] 45%|████▌     | 5/11 [16:30<19:48, 198.14s/it] 55%|█████▍    | 6/11 [19:54<16:40, 200.04s/it] 64%|██████▎   | 7/11 [23:13<13:19, 199.77s/it] 73%|███████▎  | 8/11 [26:30<09:56, 198.82s/it] 82%|████████▏ | 9/11 [29:59<06:44, 202.20s/it] 91%|█████████ | 10/11 [33:15<03:20, 200.19s/it]100%|██████████| 11/11 [35:23<00:00, 178.14s/it]run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
100%|██████████| 11/11 [35:23<00:00, 193.05s/it]
01/25/2022 07:03:53 - INFO - __main__ -     bleu-4 = 21.32 
01/25/2022 07:03:53 - INFO - __main__ -     xMatch = 0.0 
01/25/2022 07:03:53 - INFO - __main__ -     ********************
01/25/2022 07:03:53 - INFO - __main__ -   Test file: dataset/evosuit/Evosuit_test_lang3_ArrayUtils.methods,dataset/evosuit/Evosuit_test_lang3_ArrayUtils.tests
  0%|          | 0/682 [00:00<?, ?it/s]  2%|▏         | 13/682 [00:00<00:05, 128.84it/s]  4%|▍         | 27/682 [00:00<00:04, 132.02it/s]  6%|▋         | 43/682 [00:00<00:04, 137.56it/s]  8%|▊         | 57/682 [00:00<00:04, 126.92it/s] 11%|█         | 72/682 [00:00<00:04, 134.57it/s] 13%|█▎        | 89/682 [00:00<00:04, 144.79it/s] 16%|█▌        | 106/682 [00:00<00:03, 150.37it/s] 18%|█▊        | 122/682 [00:00<00:04, 125.25it/s] 20%|█▉        | 136/682 [00:01<00:04, 115.15it/s] 22%|██▏       | 149/682 [00:01<00:05, 105.46it/s] 26%|██▌       | 176/682 [00:01<00:03, 144.69it/s] 28%|██▊       | 192/682 [00:01<00:03, 147.23it/s] 32%|███▏      | 215/682 [00:01<00:02, 167.43it/s] 34%|███▍      | 233/682 [00:01<00:03, 147.64it/s] 37%|███▋      | 249/682 [00:01<00:03, 128.82it/s] 39%|███▊      | 263/682 [00:01<00:03, 127.97it/s] 41%|████      | 277/682 [00:02<00:03, 115.03it/s] 43%|████▎     | 290/682 [00:02<00:03, 110.28it/s] 46%|████▌     | 313/682 [00:02<00:02, 136.12it/s] 48%|████▊     | 328/682 [00:02<00:02, 127.75it/s] 50%|█████     | 342/682 [00:02<00:02, 120.51it/s] 52%|█████▏    | 355/682 [00:02<00:02, 122.59it/s] 54%|█████▍    | 368/682 [00:02<00:03, 103.46it/s] 56%|█████▌    | 382/682 [00:03<00:02, 111.30it/s] 58%|█████▊    | 396/682 [00:03<00:02, 116.81it/s] 61%|██████    | 413/682 [00:03<00:02, 128.79it/s] 63%|██████▎   | 429/682 [00:03<00:01, 135.09it/s] 65%|██████▌   | 446/682 [00:03<00:01, 143.99it/s] 68%|██████▊   | 464/682 [00:03<00:01, 151.95it/s] 71%|███████   | 481/682 [00:03<00:01, 156.21it/s] 73%|███████▎  | 499/682 [00:03<00:01, 159.17it/s] 76%|███████▌  | 517/682 [00:03<00:01, 163.26it/s] 78%|███████▊  | 534/682 [00:03<00:00, 153.50it/s] 81%|████████  | 554/682 [00:04<00:00, 165.39it/s] 86%|████████▌ | 584/682 [00:04<00:00, 193.21it/s] 89%|████████▊ | 604/682 [00:04<00:00, 169.47it/s] 91%|█████████ | 622/682 [00:04<00:00, 142.80it/s] 94%|█████████▎| 638/682 [00:04<00:00, 136.05it/s] 96%|█████████▌| 653/682 [00:04<00:00, 133.15it/s] 98%|█████████▊| 667/682 [00:04<00:00, 129.63it/s]100%|█████████▉| 681/682 [00:05<00:00, 124.87it/s]100%|██████████| 682/682 [00:05<00:00, 135.27it/s]
  0%|          | 0/11 [00:00<?, ?it/s]  9%|▉         | 1/11 [03:14<32:21, 194.18s/it] 18%|█▊        | 2/11 [06:37<29:53, 199.30s/it] 27%|██▋       | 3/11 [09:53<26:24, 198.01s/it] 36%|███▋      | 4/11 [13:06<22:51, 195.99s/it] 45%|████▌     | 5/11 [16:30<19:53, 198.90s/it] 55%|█████▍    | 6/11 [19:54<16:43, 200.74s/it] 64%|██████▎   | 7/11 [23:14<13:21, 200.41s/it] 73%|███████▎  | 8/11 [26:32<09:59, 199.72s/it] 82%|████████▏ | 9/11 [30:02<06:45, 202.96s/it] 91%|█████████ | 10/11 [33:18<03:20, 200.74s/it]100%|██████████| 11/11 [35:27<00:00, 178.89s/it]run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
100%|██████████| 11/11 [35:28<00:00, 193.46s/it]
01/25/2022 07:39:27 - INFO - __main__ -     bleu-4 = 21.32 
01/25/2022 07:39:27 - INFO - __main__ -     xMatch = 0.0 
01/25/2022 07:39:27 - INFO - __main__ -     ********************
