/var/spool/slurmd/job36017944/slurm_script: line 7: ./Env/bin/activate: No such file or directory
06/09/2022 02:50:41 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=10, cal_blue=1, config_name='graphcodebert-base', dev_filename='dataset/line2test/test_sorted.methods,dataset/line2test/test_sorted.tests', do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=64, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='saved_models/dec_6_line2test_data_graphcodebert_500_150/methods-tests//checkpoint-best-bleu/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=512, max_steps=-1, max_target_length=320, model_name_or_path='saved_models/dec_6_line2test_data_graphcodebert_500_150/methods-tests//checkpoint-best-bleu/pytorch_model.bin', model_type='roberta', no_cuda=False, num_train_epochs=3, output_dir='saved_models/dec_6_line2test_data_graphcodebert_500_150/methods-tests/', seed=42, source_lang='methods', test_filename='dataset/line2test/test_sorted.methods,dataset/line2test/test_sorted.tests', tokenizer_name='graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
06/09/2022 02:50:41 - INFO - __main__ -   1
Some weights of the model checkpoint at saved_models/dec_6_line2test_data_graphcodebert_500_150/methods-tests//checkpoint-best-bleu/pytorch_model.bin were not used when initializing RobertaModel: ['encoder.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.1.attention.self.value.bias', 'decoder.layers.2.self_attn.out_proj.bias', 'encoder.encoder.layer.4.attention.self.query.bias', 'encoder.encoder.layer.10.output.LayerNorm.weight', 'encoder.encoder.layer.9.attention.self.key.bias', 'encoder.encoder.layer.5.output.LayerNorm.weight', 'encoder.encoder.layer.5.output.LayerNorm.bias', 'encoder.encoder.layer.7.intermediate.dense.weight', 'encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'decoder.layers.1.linear1.bias', 'decoder.layers.4.linear1.weight', 'decoder.layers.3.multihead_attn.in_proj_bias', 'decoder.layers.5.linear2.weight', 'encoder.encoder.layer.7.attention.output.dense.weight', 'encoder.encoder.layer.8.output.dense.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.embeddings.token_type_embeddings.weight', 'encoder.encoder.layer.1.attention.self.key.weight', 'encoder.encoder.layer.11.output.LayerNorm.bias', 'encoder.encoder.layer.11.attention.output.dense.weight', 'encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.encoder.layer.5.attention.self.query.weight', 'decoder.layers.0.linear1.bias', 'encoder.encoder.layer.10.attention.self.query.bias', 'decoder.layers.0.linear2.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'decoder.layers.2.multihead_attn.out_proj.weight', 'decoder.layers.2.linear1.bias', 'encoder.encoder.layer.1.attention.self.value.weight', 'encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'decoder.layers.5.linear1.bias', 'encoder.encoder.layer.9.attention.output.dense.weight', 'decoder.layers.2.norm1.bias', 'decoder.layers.4.norm2.weight', 'decoder.layers.2.self_attn.in_proj_bias', 'encoder.encoder.layer.9.attention.output.dense.bias', 'encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.intermediate.dense.bias', 'encoder.encoder.layer.10.attention.output.dense.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.encoder.layer.0.intermediate.dense.weight', 'decoder.layers.2.linear2.weight', 'encoder.encoder.layer.9.output.dense.bias', 'encoder.encoder.layer.6.attention.self.query.weight', 'encoder.encoder.layer.8.attention.output.dense.weight', 'encoder.encoder.layer.5.output.dense.bias', 'encoder.encoder.layer.9.output.LayerNorm.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.5.multihead_attn.in_proj_weight', 'encoder.encoder.layer.6.intermediate.dense.weight', 'decoder.layers.5.norm3.weight', 'encoder.encoder.layer.3.attention.self.key.bias', 'encoder.encoder.layer.9.output.dense.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.encoder.layer.9.attention.self.value.bias', 'decoder.layers.3.linear2.bias', 'decoder.layers.0.multihead_attn.out_proj.weight', 'encoder.encoder.layer.8.output.LayerNorm.bias', 'encoder.encoder.layer.2.attention.self.key.bias', 'encoder.encoder.layer.3.output.LayerNorm.weight', 'encoder.encoder.layer.11.output.LayerNorm.weight', 'encoder.encoder.layer.11.output.dense.weight', 'decoder.layers.1.norm2.bias', 'decoder.layers.2.multihead_attn.in_proj_bias', 'decoder.layers.3.norm2.weight', 'encoder.encoder.layer.1.attention.output.dense.bias', 'encoder.encoder.layer.6.output.LayerNorm.bias', 'encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.encoder.layer.6.attention.output.dense.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'encoder.encoder.layer.0.attention.output.dense.bias', 'encoder.encoder.layer.2.intermediate.dense.bias', 'encoder.encoder.layer.3.attention.self.key.weight', 'encoder.encoder.layer.7.attention.self.query.weight', 'dense.bias', 'encoder.encoder.layer.0.attention.self.query.bias', 'encoder.encoder.layer.0.intermediate.dense.bias', 'encoder.encoder.layer.8.attention.self.key.bias', 'decoder.layers.1.linear2.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.encoder.layer.10.attention.self.value.bias', 'encoder.encoder.layer.1.output.dense.bias', 'encoder.encoder.layer.4.attention.self.key.bias', 'decoder.layers.1.linear2.weight', 'encoder.encoder.layer.7.attention.output.dense.bias', 'decoder.layers.3.self_attn.in_proj_bias', 'encoder.encoder.layer.8.output.dense.bias', 'decoder.layers.1.self_attn.in_proj_bias', 'encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'encoder.encoder.layer.10.attention.self.key.bias', 'decoder.layers.0.self_attn.out_proj.bias', 'decoder.layers.1.multihead_attn.in_proj_bias', 'bias', 'encoder.encoder.layer.2.attention.self.value.bias', 'encoder.encoder.layer.4.attention.output.dense.weight', 'encoder.encoder.layer.3.output.dense.weight', 'encoder.encoder.layer.8.attention.self.value.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.encoder.layer.11.attention.self.value.weight', 'encoder.encoder.layer.6.output.LayerNorm.weight', 'decoder.layers.2.self_attn.in_proj_weight', 'decoder.layers.4.norm1.bias', 'decoder.layers.3.linear2.weight', 'decoder.layers.4.multihead_attn.in_proj_weight', 'decoder.layers.0.norm2.bias', 'decoder.layers.2.linear2.bias', 'encoder.encoder.layer.8.intermediate.dense.bias', 'encoder.encoder.layer.5.attention.self.query.bias', 'encoder.embeddings.LayerNorm.weight', 'encoder.encoder.layer.10.output.dense.weight', 'encoder.encoder.layer.7.attention.self.query.bias', 'encoder.encoder.layer.5.output.dense.weight', 'encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.output.LayerNorm.weight', 'decoder.layers.2.multihead_attn.in_proj_weight', 'encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.encoder.layer.1.intermediate.dense.bias', 'decoder.layers.5.self_attn.in_proj_bias', 'decoder.layers.0.norm1.bias', 'encoder.encoder.layer.9.attention.self.query.bias', 'decoder.layers.1.multihead_attn.in_proj_weight', 'encoder.encoder.layer.6.attention.output.dense.bias', 'decoder.layers.4.norm2.bias', 'encoder.pooler.dense.weight', 'encoder.encoder.layer.9.attention.self.query.weight', 'decoder.layers.4.multihead_attn.in_proj_bias', 'encoder.encoder.layer.9.output.LayerNorm.bias', 'encoder.encoder.layer.9.intermediate.dense.bias', 'encoder.encoder.layer.2.output.dense.weight', 'encoder.encoder.layer.11.intermediate.dense.weight', 'encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.output.LayerNorm.bias', 'encoder.encoder.layer.9.intermediate.dense.weight', 'decoder.layers.3.multihead_attn.out_proj.weight', 'encoder.encoder.layer.0.output.dense.weight', 'decoder.layers.5.norm1.bias', 'decoder.layers.3.linear1.bias', 'encoder.encoder.layer.3.intermediate.dense.weight', 'encoder.encoder.layer.7.attention.self.key.weight', 'decoder.layers.3.self_attn.in_proj_weight', 'decoder.layers.5.norm1.weight', 'encoder.encoder.layer.10.attention.self.value.weight', 'encoder.encoder.layer.6.attention.self.query.bias', 'decoder.layers.3.linear1.weight', 'encoder.encoder.layer.8.attention.self.value.bias', 'encoder.encoder.layer.10.output.LayerNorm.bias', 'decoder.layers.4.self_attn.in_proj_weight', 'encoder.encoder.layer.7.output.dense.bias', 'encoder.encoder.layer.3.attention.self.query.bias', 'decoder.layers.0.norm1.weight', 'encoder.encoder.layer.3.attention.self.value.bias', 'encoder.encoder.layer.3.intermediate.dense.bias', 'decoder.layers.3.norm2.bias', 'decoder.layers.5.multihead_attn.out_proj.bias', 'decoder.layers.2.norm1.weight', 'decoder.layers.4.linear2.bias', 'encoder.encoder.layer.5.attention.output.dense.bias', 'decoder.layers.4.norm3.bias', 'decoder.layers.1.norm2.weight', 'decoder.layers.4.self_attn.out_proj.weight', 'encoder.encoder.layer.2.output.LayerNorm.weight', 'dense.weight', 'decoder.layers.5.linear1.weight', 'encoder.encoder.layer.8.intermediate.dense.weight', 'decoder.layers.2.multihead_attn.out_proj.bias', 'decoder.layers.2.norm2.bias', 'encoder.encoder.layer.7.output.LayerNorm.weight', 'encoder.encoder.layer.1.attention.self.query.bias', 'encoder.encoder.layer.2.output.LayerNorm.bias', 'encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.output.dense.bias', 'encoder.encoder.layer.5.attention.output.dense.weight', 'encoder.encoder.layer.3.attention.self.value.weight', 'encoder.encoder.layer.1.output.LayerNorm.bias', 'encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.encoder.layer.11.attention.self.query.weight', 'decoder.layers.0.linear1.weight', 'decoder.layers.5.linear2.bias', 'encoder.encoder.layer.2.attention.self.query.weight', 'decoder.layers.0.multihead_attn.in_proj_bias', 'decoder.layers.3.norm1.bias', 'encoder.encoder.layer.1.output.LayerNorm.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'encoder.encoder.layer.5.attention.self.key.weight', 'decoder.layers.0.self_attn.in_proj_weight', 'encoder.encoder.layer.0.output.LayerNorm.bias', 'encoder.encoder.layer.4.attention.output.dense.bias', 'encoder.encoder.layer.1.intermediate.dense.weight', 'decoder.layers.4.self_attn.in_proj_bias', 'encoder.encoder.layer.1.attention.output.dense.weight', 'encoder.encoder.layer.3.output.LayerNorm.bias', 'decoder.layers.0.norm2.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'encoder.encoder.layer.6.attention.self.value.weight', 'encoder.encoder.layer.5.intermediate.dense.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.encoder.layer.10.intermediate.dense.weight', 'decoder.layers.5.norm3.bias', 'encoder.encoder.layer.6.output.dense.weight', 'decoder.layers.1.norm1.weight', 'encoder.encoder.layer.4.attention.self.key.weight', 'decoder.layers.2.norm3.bias', 'encoder.encoder.layer.0.attention.self.key.weight', 'encoder.encoder.layer.9.attention.self.value.weight', 'encoder.encoder.layer.9.attention.self.key.weight', 'encoder.encoder.layer.5.intermediate.dense.bias', 'encoder.encoder.layer.6.output.dense.bias', 'encoder.encoder.layer.6.attention.self.key.weight', 'encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'decoder.layers.3.norm1.weight', 'encoder.encoder.layer.0.attention.self.value.weight', 'encoder.encoder.layer.11.attention.output.dense.bias', 'decoder.layers.4.norm1.weight', 'encoder.encoder.layer.2.attention.self.key.weight', 'encoder.encoder.layer.5.attention.self.key.bias', 'encoder.encoder.layer.4.attention.self.value.bias', 'encoder.encoder.layer.8.attention.self.query.bias', 'encoder.encoder.layer.8.attention.output.dense.bias', 'encoder.encoder.layer.11.attention.self.key.bias', 'decoder.layers.1.norm3.bias', 'encoder.encoder.layer.7.attention.self.key.bias', 'encoder.embeddings.LayerNorm.bias', 'encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.encoder.layer.10.attention.self.query.weight', 'decoder.layers.5.self_attn.in_proj_weight', 'encoder.encoder.layer.11.attention.self.key.weight', 'encoder.encoder.layer.3.attention.output.dense.bias', 'decoder.layers.0.norm3.weight', 'encoder.encoder.layer.2.intermediate.dense.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.5.norm2.weight', 'encoder.encoder.layer.0.output.dense.bias', 'encoder.encoder.layer.2.output.dense.bias', 'encoder.encoder.layer.2.attention.self.query.bias', 'encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.encoder.layer.0.attention.self.query.weight', 'encoder.encoder.layer.10.attention.output.dense.bias', 'decoder.layers.1.norm3.weight', 'decoder.layers.1.self_attn.in_proj_weight', 'encoder.encoder.layer.7.output.dense.weight', 'lm_head.weight', 'encoder.encoder.layer.4.attention.self.value.weight', 'decoder.layers.1.linear1.weight', 'encoder.encoder.layer.3.attention.self.query.weight', 'encoder.encoder.layer.7.intermediate.dense.bias', 'encoder.encoder.layer.0.attention.output.dense.weight', 'encoder.encoder.layer.10.attention.self.key.weight', 'encoder.encoder.layer.6.intermediate.dense.bias', 'encoder.encoder.layer.0.attention.self.value.bias', 'decoder.layers.5.self_attn.out_proj.bias', 'encoder.encoder.layer.3.output.dense.bias', 'encoder.encoder.layer.7.output.LayerNorm.bias', 'encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'decoder.layers.4.linear2.weight', 'encoder.encoder.layer.4.output.dense.weight', 'encoder.encoder.layer.3.attention.output.dense.weight', 'decoder.layers.3.norm3.bias', 'encoder.embeddings.word_embeddings.weight', 'encoder.encoder.layer.0.attention.self.key.bias', 'encoder.encoder.layer.6.attention.self.value.bias', 'decoder.layers.4.self_attn.out_proj.bias', 'encoder.encoder.layer.8.attention.self.query.weight', 'encoder.encoder.layer.10.output.dense.bias', 'decoder.layers.0.norm3.bias', 'decoder.layers.3.multihead_attn.out_proj.bias', 'decoder.layers.5.multihead_attn.out_proj.weight', 'decoder.layers.0.self_attn.in_proj_bias', 'encoder.encoder.layer.6.attention.self.key.bias', 'decoder.layers.2.linear1.weight', 'decoder.layers.4.norm3.weight', 'encoder.encoder.layer.0.output.LayerNorm.weight', 'decoder.layers.5.norm2.bias', 'decoder.layers.0.multihead_attn.out_proj.bias', 'encoder.encoder.layer.10.intermediate.dense.bias', 'encoder.pooler.dense.bias', 'encoder.encoder.layer.2.attention.output.dense.weight', 'encoder.encoder.layer.1.attention.self.key.bias', 'decoder.layers.1.multihead_attn.out_proj.weight', 'encoder.encoder.layer.8.output.LayerNorm.weight', 'decoder.layers.4.linear1.bias', 'encoder.encoder.layer.2.attention.output.dense.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.multihead_attn.out_proj.bias', 'decoder.layers.2.norm2.weight', 'encoder.encoder.layer.7.attention.self.value.bias', 'encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.self.query.weight', 'decoder.layers.3.norm3.weight', 'encoder.encoder.layer.4.intermediate.dense.weight', 'encoder.encoder.layer.5.attention.self.value.weight', 'encoder.encoder.layer.4.attention.self.query.weight', 'decoder.layers.0.linear2.bias', 'decoder.layers.0.multihead_attn.in_proj_weight', 'encoder.encoder.layer.11.intermediate.dense.bias', 'decoder.layers.3.multihead_attn.in_proj_weight', 'encoder.encoder.layer.11.attention.self.query.bias', 'encoder.encoder.layer.11.attention.self.value.bias', 'decoder.layers.4.multihead_attn.out_proj.weight', 'encoder.encoder.layer.8.attention.self.key.weight', 'decoder.layers.2.norm3.weight', 'encoder.encoder.layer.11.output.dense.bias', 'decoder.layers.4.multihead_attn.out_proj.bias', 'encoder.encoder.layer.5.attention.self.value.bias', 'encoder.encoder.layer.7.attention.self.value.weight', 'decoder.layers.5.multihead_attn.in_proj_bias', 'encoder.embeddings.position_ids', 'decoder.layers.1.norm1.bias', 'encoder.encoder.layer.1.output.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at saved_models/dec_6_line2test_data_graphcodebert_500_150/methods-tests//checkpoint-best-bleu/pytorch_model.bin and are newly initialized: ['encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'pooler.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'pooler.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/09/2022 02:50:51 - INFO - __main__ -   reload model from saved_models/dec_6_line2test_data_graphcodebert_500_150/methods-tests//checkpoint-best-bleu/pytorch_model.bin
06/09/2022 02:50:55 - INFO - __main__ -   Test file: dataset/line2test/test_sorted.methods,dataset/line2test/test_sorted.tests
  0%|          | 0/1977 [00:00<?, ?it/s]  0%|          | 5/1977 [00:00<00:46, 42.29it/s]  1%|          | 10/1977 [00:00<00:44, 44.32it/s]  1%|          | 17/1977 [00:00<00:35, 54.91it/s]  1%|          | 23/1977 [00:00<00:34, 56.67it/s]  1%|▏         | 29/1977 [00:00<00:43, 45.21it/s]  2%|▏         | 34/1977 [00:00<01:06, 29.29it/s]  2%|▏         | 38/1977 [00:01<01:06, 28.98it/s]  2%|▏         | 44/1977 [00:01<00:55, 34.77it/s]  3%|▎         | 51/1977 [00:01<00:48, 40.12it/s]  3%|▎         | 56/1977 [00:01<00:48, 39.45it/s]  3%|▎         | 68/1977 [00:01<00:33, 56.93it/s]  4%|▍         | 75/1977 [00:01<00:35, 52.99it/s]  4%|▍         | 81/1977 [00:01<00:38, 49.25it/s]  4%|▍         | 87/1977 [00:02<00:45, 41.22it/s]  5%|▍         | 94/1977 [00:02<00:39, 47.08it/s]  5%|▌         | 105/1977 [00:02<00:30, 60.99it/s]  6%|▌         | 113/1977 [00:02<00:28, 65.23it/s]  7%|▋         | 131/1977 [00:02<00:21, 84.94it/s]  7%|▋         | 140/1977 [00:03<00:44, 40.86it/s]  7%|▋         | 147/1977 [00:03<01:25, 21.33it/s]  8%|▊         | 152/1977 [00:04<01:30, 20.26it/s]  8%|▊         | 160/1977 [00:04<01:10, 25.71it/s]  8%|▊         | 168/1977 [00:04<00:56, 32.03it/s]  9%|▉         | 176/1977 [00:04<00:46, 38.62it/s]  9%|▉         | 183/1977 [00:04<00:48, 36.80it/s] 10%|▉         | 189/1977 [00:05<01:11, 24.89it/s] 10%|▉         | 194/1977 [00:05<01:09, 25.79it/s] 11%|█         | 208/1977 [00:05<00:44, 39.96it/s] 11%|█         | 214/1977 [00:05<00:44, 39.77it/s] 11%|█▏        | 224/1977 [00:05<00:35, 49.14it/s] 12%|█▏        | 233/1977 [00:05<00:31, 56.24it/s] 12%|█▏        | 242/1977 [00:06<00:27, 62.32it/s] 13%|█▎        | 250/1977 [00:06<00:28, 59.89it/s] 13%|█▎        | 257/1977 [00:06<00:32, 53.48it/s] 14%|█▎        | 271/1977 [00:06<00:23, 71.19it/s] 14%|█▍        | 280/1977 [00:06<00:30, 56.48it/s] 15%|█▍        | 287/1977 [00:06<00:39, 43.10it/s] 15%|█▍        | 293/1977 [00:07<00:50, 33.10it/s] 15%|█▌        | 298/1977 [00:07<00:59, 28.41it/s] 15%|█▌        | 302/1977 [00:07<01:04, 25.84it/s] 15%|█▌        | 306/1977 [00:08<01:15, 22.18it/s] 16%|█▌        | 311/1977 [00:08<01:05, 25.60it/s] 16%|█▌        | 315/1977 [00:08<00:59, 27.77it/s] 16%|█▌        | 320/1977 [00:08<00:53, 31.16it/s] 16%|█▋        | 325/1977 [00:08<00:48, 33.88it/s] 17%|█▋        | 330/1977 [00:08<00:45, 36.33it/s] 17%|█▋        | 335/1977 [00:08<00:42, 38.36it/s] 17%|█▋        | 340/1977 [00:08<00:41, 39.81it/s] 17%|█▋        | 345/1977 [00:08<00:39, 41.14it/s] 18%|█▊        | 351/1977 [00:09<00:36, 44.91it/s] 18%|█▊        | 357/1977 [00:09<00:34, 46.51it/s] 18%|█▊        | 362/1977 [00:09<00:37, 43.20it/s] 19%|█▊        | 367/1977 [00:09<00:37, 42.85it/s] 19%|█▉        | 380/1977 [00:09<00:24, 64.65it/s] 20%|█▉        | 388/1977 [00:09<00:23, 68.28it/s] 20%|██        | 400/1977 [00:09<00:19, 82.13it/s] 21%|██        | 409/1977 [00:10<00:32, 48.21it/s] 21%|██        | 418/1977 [00:10<00:28, 54.72it/s] 22%|██▏       | 426/1977 [00:10<00:28, 54.57it/s] 22%|██▏       | 433/1977 [00:10<00:48, 31.83it/s] 22%|██▏       | 439/1977 [00:11<00:45, 34.14it/s] 23%|██▎       | 445/1977 [00:11<00:40, 37.61it/s] 23%|██▎       | 451/1977 [00:11<00:47, 32.17it/s] 23%|██▎       | 456/1977 [00:11<00:53, 28.61it/s] 23%|██▎       | 460/1977 [00:11<00:55, 27.49it/s] 23%|██▎       | 464/1977 [00:11<00:54, 27.52it/s] 24%|██▍       | 471/1977 [00:12<00:44, 33.93it/s] 24%|██▍       | 475/1977 [00:12<00:47, 31.33it/s] 24%|██▍       | 483/1977 [00:12<00:36, 40.57it/s] 25%|██▍       | 489/1977 [00:12<00:34, 43.42it/s] 25%|██▌       | 498/1977 [00:12<00:27, 53.96it/s] 26%|██▌       | 508/1977 [00:12<00:23, 63.18it/s] 26%|██▌       | 516/1977 [00:12<00:22, 65.99it/s] 27%|██▋       | 539/1977 [00:12<00:13, 108.42it/s] 28%|██▊       | 551/1977 [00:13<00:13, 104.47it/s] 28%|██▊       | 562/1977 [00:13<00:15, 93.98it/s]  29%|██▉       | 572/1977 [00:13<00:17, 80.39it/s] 30%|██▉       | 587/1977 [00:13<00:16, 86.82it/s] 30%|███       | 597/1977 [00:13<00:22, 60.57it/s] 31%|███       | 608/1977 [00:13<00:20, 67.91it/s] 31%|███       | 617/1977 [00:14<00:19, 71.29it/s] 32%|███▏      | 626/1977 [00:14<00:38, 35.25it/s] 32%|███▏      | 633/1977 [00:15<00:52, 25.45it/s] 33%|███▎      | 644/1977 [00:15<00:38, 34.35it/s] 33%|███▎      | 651/1977 [00:15<00:34, 38.08it/s] 33%|███▎      | 659/1977 [00:15<00:29, 44.19it/s] 34%|███▎      | 667/1977 [00:15<00:26, 49.93it/s] 34%|███▍      | 675/1977 [00:15<00:23, 55.34it/s] 35%|███▌      | 692/1977 [00:15<00:16, 79.24it/s] 36%|███▌      | 709/1977 [00:15<00:12, 97.66it/s] 37%|███▋      | 723/1977 [00:16<00:11, 106.60it/s] 37%|███▋      | 735/1977 [00:16<00:11, 103.84it/s] 38%|███▊      | 747/1977 [00:16<00:15, 81.20it/s]  38%|███▊      | 757/1977 [00:16<00:15, 80.85it/s] 39%|███▉      | 767/1977 [00:16<00:15, 77.30it/s] 39%|███▉      | 776/1977 [00:16<00:16, 71.40it/s] 40%|███▉      | 784/1977 [00:16<00:17, 69.03it/s] 40%|████      | 792/1977 [00:17<00:16, 70.55it/s] 40%|████      | 800/1977 [00:17<00:16, 70.95it/s] 41%|████      | 808/1977 [00:17<00:17, 65.75it/s] 41%|████      | 815/1977 [00:17<00:18, 64.42it/s] 42%|████▏     | 822/1977 [00:17<00:17, 64.44it/s] 42%|████▏     | 829/1977 [00:17<00:17, 64.27it/s] 42%|████▏     | 836/1977 [00:17<00:17, 64.95it/s] 43%|████▎     | 843/1977 [00:17<00:17, 64.59it/s] 43%|████▎     | 852/1977 [00:17<00:16, 66.71it/s] 43%|████▎     | 859/1977 [00:18<00:18, 61.61it/s] 44%|████▍     | 866/1977 [00:18<00:17, 62.40it/s] 44%|████▍     | 873/1977 [00:18<00:17, 63.04it/s] 45%|████▍     | 881/1977 [00:18<00:16, 66.38it/s] 45%|████▍     | 888/1977 [00:18<00:17, 63.91it/s] 45%|████▌     | 896/1977 [00:18<00:16, 66.79it/s] 46%|████▌     | 903/1977 [00:19<00:27, 39.53it/s] 46%|████▌     | 909/1977 [00:19<00:32, 33.06it/s] 46%|████▌     | 914/1977 [00:19<00:35, 30.19it/s] 46%|████▋     | 918/1977 [00:19<00:37, 27.96it/s] 47%|████▋     | 922/1977 [00:19<00:39, 26.81it/s] 47%|████▋     | 926/1977 [00:20<00:42, 24.86it/s] 47%|████▋     | 929/1977 [00:20<00:40, 25.68it/s] 47%|████▋     | 932/1977 [00:20<00:41, 25.33it/s] 47%|████▋     | 935/1977 [00:20<00:41, 25.11it/s] 48%|████▊     | 940/1977 [00:20<00:33, 30.71it/s] 48%|████▊     | 947/1977 [00:20<00:25, 40.12it/s] 48%|████▊     | 953/1977 [00:20<00:24, 42.25it/s] 48%|████▊     | 958/1977 [00:20<00:25, 40.70it/s] 49%|████▊     | 963/1977 [00:20<00:23, 42.46it/s] 49%|████▉     | 971/1977 [00:21<00:19, 50.86it/s] 50%|████▉     | 981/1977 [00:21<00:15, 63.39it/s] 50%|█████     | 989/1977 [00:21<00:16, 58.92it/s] 50%|█████     | 996/1977 [00:21<00:33, 29.25it/s] 51%|█████     | 1001/1977 [00:22<00:36, 26.59it/s] 51%|█████     | 1012/1977 [00:22<00:24, 38.81it/s] 51%|█████▏    | 1018/1977 [00:22<00:31, 30.14it/s] 52%|█████▏    | 1023/1977 [00:22<00:30, 31.44it/s] 52%|█████▏    | 1030/1977 [00:22<00:25, 37.56it/s] 52%|█████▏    | 1036/1977 [00:22<00:24, 38.10it/s] 53%|█████▎    | 1041/1977 [00:23<00:24, 38.86it/s] 53%|█████▎    | 1046/1977 [00:23<00:23, 39.42it/s] 53%|█████▎    | 1051/1977 [00:23<00:23, 39.69it/s] 53%|█████▎    | 1056/1977 [00:23<00:23, 40.02it/s] 54%|█████▎    | 1062/1977 [00:23<00:20, 43.66it/s] 54%|█████▍    | 1074/1977 [00:23<00:14, 61.93it/s] 55%|█████▍    | 1084/1977 [00:23<00:12, 70.98it/s] 55%|█████▌    | 1092/1977 [00:23<00:13, 66.47it/s] 56%|█████▌    | 1102/1977 [00:24<00:11, 73.13it/s] 56%|█████▌    | 1110/1977 [00:24<00:23, 36.55it/s] 56%|█████▋    | 1116/1977 [00:24<00:31, 27.17it/s] 57%|█████▋    | 1121/1977 [00:25<00:38, 22.39it/s] 57%|█████▋    | 1125/1977 [00:25<00:39, 21.84it/s] 57%|█████▋    | 1131/1977 [00:25<00:32, 25.75it/s] 57%|█████▋    | 1135/1977 [00:25<00:32, 25.95it/s] 58%|█████▊    | 1144/1977 [00:25<00:23, 35.44it/s] 58%|█████▊    | 1149/1977 [00:26<00:27, 29.77it/s] 58%|█████▊    | 1153/1977 [00:26<00:30, 26.89it/s] 59%|█████▊    | 1157/1977 [00:26<00:43, 18.67it/s] 59%|█████▊    | 1160/1977 [00:27<00:53, 15.24it/s] 59%|█████▉    | 1163/1977 [00:27<00:52, 15.49it/s] 59%|█████▉    | 1165/1977 [00:27<00:51, 15.82it/s] 59%|█████▉    | 1167/1977 [00:27<00:49, 16.31it/s] 59%|█████▉    | 1169/1977 [00:27<00:48, 16.74it/s] 59%|█████▉    | 1171/1977 [00:27<00:46, 17.16it/s] 59%|█████▉    | 1173/1977 [00:27<00:45, 17.50it/s] 59%|█████▉    | 1175/1977 [00:27<00:45, 17.73it/s] 60%|█████▉    | 1180/1977 [00:28<00:31, 25.43it/s] 60%|█████▉    | 1185/1977 [00:28<00:25, 30.73it/s] 60%|██████    | 1189/1977 [00:28<00:29, 26.84it/s] 60%|██████    | 1192/1977 [00:28<00:31, 25.09it/s] 60%|██████    | 1196/1977 [00:28<00:28, 27.60it/s] 61%|██████    | 1202/1977 [00:28<00:23, 33.38it/s] 61%|██████    | 1206/1977 [00:28<00:22, 34.04it/s] 61%|██████▏   | 1215/1977 [00:28<00:15, 47.98it/s] 62%|██████▏   | 1229/1977 [00:29<00:10, 71.21it/s] 63%|██████▎   | 1237/1977 [00:29<00:14, 51.47it/s] 63%|██████▎   | 1244/1977 [00:29<00:18, 39.57it/s] 64%|██████▎   | 1257/1977 [00:29<00:12, 55.65it/s] 64%|██████▍   | 1265/1977 [00:29<00:15, 45.56it/s] 64%|██████▍   | 1272/1977 [00:30<00:16, 41.84it/s] 65%|██████▍   | 1281/1977 [00:30<00:14, 48.63it/s] 65%|██████▌   | 1287/1977 [00:30<00:14, 48.72it/s] 65%|██████▌   | 1293/1977 [00:30<00:14, 48.62it/s] 66%|██████▌   | 1301/1977 [00:30<00:12, 55.47it/s] 66%|██████▋   | 1312/1977 [00:30<00:09, 67.55it/s] 67%|██████▋   | 1320/1977 [00:30<00:09, 66.19it/s] 67%|██████▋   | 1329/1977 [00:30<00:09, 69.19it/s] 68%|██████▊   | 1337/1977 [00:31<00:11, 54.15it/s] 68%|██████▊   | 1344/1977 [00:31<00:13, 48.61it/s] 68%|██████▊   | 1350/1977 [00:31<00:14, 44.15it/s] 69%|██████▊   | 1355/1977 [00:31<00:14, 44.41it/s] 69%|██████▉   | 1362/1977 [00:31<00:12, 48.48it/s] 69%|██████▉   | 1370/1977 [00:31<00:11, 54.85it/s] 70%|██████▉   | 1376/1977 [00:32<00:12, 47.93it/s] 70%|██████▉   | 1382/1977 [00:32<00:14, 41.47it/s] 70%|███████   | 1387/1977 [00:32<00:17, 33.14it/s] 70%|███████   | 1391/1977 [00:32<00:21, 27.56it/s] 71%|███████   | 1395/1977 [00:33<00:25, 23.09it/s] 71%|███████   | 1398/1977 [00:33<00:27, 20.89it/s] 71%|███████   | 1401/1977 [00:33<00:29, 19.26it/s] 71%|███████   | 1404/1977 [00:33<00:27, 20.51it/s] 71%|███████▏  | 1409/1977 [00:33<00:22, 25.40it/s] 72%|███████▏  | 1415/1977 [00:33<00:17, 31.57it/s] 72%|███████▏  | 1421/1977 [00:33<00:15, 36.44it/s] 72%|███████▏  | 1427/1977 [00:33<00:13, 39.62it/s] 72%|███████▏  | 1432/1977 [00:34<00:13, 39.37it/s] 73%|███████▎  | 1439/1977 [00:34<00:11, 45.12it/s] 73%|███████▎  | 1444/1977 [00:34<00:13, 40.32it/s] 73%|███████▎  | 1449/1977 [00:34<00:21, 24.13it/s] 73%|███████▎  | 1453/1977 [00:35<00:29, 18.02it/s] 74%|███████▎  | 1456/1977 [00:35<00:33, 15.62it/s] 74%|███████▍  | 1460/1977 [00:35<00:28, 18.30it/s] 74%|███████▍  | 1464/1977 [00:35<00:24, 21.14it/s] 74%|███████▍  | 1469/1977 [00:35<00:19, 26.00it/s] 75%|███████▍  | 1473/1977 [00:35<00:18, 27.58it/s] 75%|███████▍  | 1478/1977 [00:36<00:16, 31.14it/s] 75%|███████▌  | 1483/1977 [00:36<00:14, 34.08it/s] 75%|███████▌  | 1487/1977 [00:36<00:13, 35.18it/s] 76%|███████▌  | 1496/1977 [00:36<00:09, 48.25it/s] 76%|███████▌  | 1502/1977 [00:36<00:09, 50.44it/s] 76%|███████▋  | 1510/1977 [00:36<00:08, 56.91it/s] 77%|███████▋  | 1516/1977 [00:36<00:12, 37.69it/s] 77%|███████▋  | 1521/1977 [00:37<00:12, 37.26it/s] 77%|███████▋  | 1526/1977 [00:37<00:12, 35.42it/s] 77%|███████▋  | 1531/1977 [00:37<00:15, 28.11it/s] 78%|███████▊  | 1535/1977 [00:37<00:16, 26.78it/s] 78%|███████▊  | 1539/1977 [00:37<00:17, 25.64it/s] 78%|███████▊  | 1542/1977 [00:38<00:18, 23.45it/s] 78%|███████▊  | 1545/1977 [00:38<00:22, 19.51it/s] 78%|███████▊  | 1548/1977 [00:38<00:24, 17.48it/s] 78%|███████▊  | 1551/1977 [00:38<00:21, 19.44it/s] 79%|███████▉  | 1562/1977 [00:38<00:11, 36.80it/s] 80%|███████▉  | 1578/1977 [00:38<00:06, 61.52it/s] 80%|████████  | 1586/1977 [00:38<00:06, 62.85it/s] 81%|████████  | 1594/1977 [00:39<00:06, 58.16it/s] 81%|████████  | 1601/1977 [00:39<00:07, 51.64it/s] 81%|████████▏ | 1607/1977 [00:39<00:06, 53.43it/s] 82%|████████▏ | 1613/1977 [00:39<00:06, 54.51it/s] 82%|████████▏ | 1623/1977 [00:39<00:05, 64.05it/s] 83%|████████▎ | 1632/1977 [00:39<00:04, 70.11it/s] 83%|████████▎ | 1641/1977 [00:39<00:04, 74.14it/s] 84%|████████▎ | 1651/1977 [00:39<00:04, 80.14it/s] 84%|████████▍ | 1661/1977 [00:40<00:03, 84.92it/s] 85%|████████▍ | 1671/1977 [00:40<00:03, 76.65it/s] 85%|████████▍ | 1679/1977 [00:41<00:12, 24.72it/s] 85%|████████▌ | 1685/1977 [00:41<00:11, 24.49it/s] 86%|████████▌ | 1699/1977 [00:41<00:07, 37.60it/s] 86%|████████▋ | 1710/1977 [00:41<00:05, 46.72it/s] 87%|████████▋ | 1719/1977 [00:41<00:04, 52.67it/s] 87%|████████▋ | 1729/1977 [00:41<00:04, 60.62it/s] 88%|████████▊ | 1738/1977 [00:42<00:04, 52.89it/s] 89%|████████▉ | 1756/1977 [00:42<00:02, 75.45it/s] 89%|████████▉ | 1766/1977 [00:42<00:02, 80.41it/s] 90%|████████▉ | 1776/1977 [00:42<00:02, 80.48it/s] 90%|█████████ | 1786/1977 [00:42<00:02, 72.08it/s] 91%|█████████ | 1795/1977 [00:42<00:02, 73.88it/s] 91%|█████████▏| 1806/1977 [00:42<00:02, 81.72it/s] 92%|█████████▏| 1815/1977 [00:42<00:02, 71.26it/s] 92%|█████████▏| 1823/1977 [00:43<00:02, 63.70it/s] 93%|█████████▎| 1833/1977 [00:43<00:02, 70.91it/s] 93%|█████████▎| 1841/1977 [00:43<00:01, 70.25it/s] 94%|█████████▎| 1849/1977 [00:43<00:02, 62.55it/s] 94%|█████████▍| 1858/1977 [00:43<00:01, 67.64it/s] 94%|█████████▍| 1866/1977 [00:43<00:01, 68.15it/s] 95%|█████████▍| 1874/1977 [00:43<00:01, 62.31it/s] 95%|█████████▌| 1884/1977 [00:43<00:01, 69.75it/s] 96%|█████████▌| 1898/1977 [00:44<00:00, 86.25it/s] 97%|█████████▋| 1908/1977 [00:44<00:00, 80.71it/s] 97%|█████████▋| 1917/1977 [00:44<00:00, 76.95it/s] 97%|█████████▋| 1925/1977 [00:44<00:01, 51.25it/s] 98%|█████████▊| 1932/1977 [00:44<00:00, 47.25it/s] 98%|█████████▊| 1939/1977 [00:45<00:00, 45.60it/s] 98%|█████████▊| 1945/1977 [00:45<00:01, 28.95it/s] 99%|█████████▊| 1950/1977 [00:45<00:01, 24.60it/s] 99%|█████████▉| 1955/1977 [00:45<00:00, 27.39it/s] 99%|█████████▉| 1959/1977 [00:46<00:00, 28.15it/s] 99%|█████████▉| 1963/1977 [00:46<00:00, 29.87it/s] 99%|█████████▉| 1967/1977 [00:46<00:00, 29.84it/s]100%|██████████| 1977/1977 [00:46<00:00, 44.16it/s]100%|██████████| 1977/1977 [00:46<00:00, 42.65it/s]
  0%|          | 0/31 [00:00<?, ?it/s]/project/6025819/sepehr8/test_generation/Graph_codeBERT_TestGen/model.py:175: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  prevK = bestScoresId // numWords
  3%|▎         | 1/31 [01:27<43:37, 87.26s/it]  6%|▋         | 2/31 [02:45<39:38, 82.02s/it] 10%|▉         | 3/31 [04:03<37:27, 80.27s/it] 13%|█▎        | 4/31 [05:22<35:47, 79.54s/it] 16%|█▌        | 5/31 [06:40<34:20, 79.25s/it] 19%|█▉        | 6/31 [07:59<32:56, 79.05s/it] 23%|██▎       | 7/31 [09:17<31:29, 78.74s/it] 26%|██▌       | 8/31 [10:37<30:15, 78.93s/it] 29%|██▉       | 9/31 [11:55<28:50, 78.66s/it] 32%|███▏      | 10/31 [13:13<27:30, 78.58s/it] 35%|███▌      | 11/31 [14:29<25:58, 77.92s/it] 39%|███▊      | 12/31 [15:49<24:47, 78.28s/it] 42%|████▏     | 13/31 [17:07<23:29, 78.32s/it] 45%|████▌     | 14/31 [18:26<22:13, 78.45s/it] 48%|████▊     | 15/31 [19:44<20:54, 78.44s/it] 52%|█████▏    | 16/31 [21:00<19:25, 77.73s/it] 55%|█████▍    | 17/31 [22:19<18:12, 78.03s/it] 58%|█████▊    | 18/31 [23:37<16:56, 78.18s/it] 61%|██████▏   | 19/31 [24:56<15:39, 78.28s/it] 65%|██████▍   | 20/31 [26:13<14:16, 77.85s/it] 68%|██████▊   | 21/31 [27:31<13:00, 78.08s/it] 71%|███████   | 22/31 [28:50<11:44, 78.25s/it] 74%|███████▍  | 23/31 [30:08<10:25, 78.17s/it] 77%|███████▋  | 24/31 [31:26<09:06, 78.11s/it] 81%|████████  | 25/31 [32:44<07:48, 78.11s/it] 84%|████████▍ | 26/31 [34:02<06:30, 78.13s/it] 87%|████████▋ | 27/31 [35:19<05:10, 77.74s/it] 90%|█████████ | 28/31 [36:37<03:53, 77.81s/it] 94%|█████████▎| 29/31 [37:56<02:35, 77.99s/it] 97%|█████████▋| 30/31 [39:13<01:17, 77.98s/it]100%|██████████| 31/31 [40:23<00:00, 75.52s/it]run.py:270: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:270: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:270: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:270: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
100%|██████████| 31/31 [40:23<00:00, 78.19s/it]
06/09/2022 03:32:08 - INFO - __main__ -     bleu-4 = 1.71 
06/09/2022 03:32:08 - INFO - __main__ -     xMatch = 5.0076 
06/09/2022 03:32:08 - INFO - __main__ -     ********************
06/09/2022 03:32:08 - INFO - __main__ -   Test file: dataset/line2test/test_sorted.methods,dataset/line2test/test_sorted.tests
  0%|          | 0/1977 [00:00<?, ?it/s]  0%|          | 1/1977 [00:00<09:44,  3.38it/s]  0%|          | 4/1977 [00:00<02:53, 11.34it/s]  0%|          | 8/1977 [00:00<01:46, 18.51it/s]  1%|          | 16/1977 [00:00<00:54, 35.95it/s]  1%|          | 23/1977 [00:00<00:44, 44.05it/s]  1%|▏         | 29/1977 [00:00<00:48, 39.83it/s]  2%|▏         | 34/1977 [00:01<01:09, 27.77it/s]  2%|▏         | 38/1977 [00:01<01:09, 27.89it/s]  2%|▏         | 44/1977 [00:01<00:56, 34.06it/s]  3%|▎         | 51/1977 [00:01<00:48, 39.76it/s]  3%|▎         | 56/1977 [00:01<00:48, 39.36it/s]  3%|▎         | 68/1977 [00:01<00:32, 57.91it/s]  4%|▍         | 75/1977 [00:02<00:35, 54.10it/s]  4%|▍         | 81/1977 [00:02<00:37, 50.24it/s]  4%|▍         | 87/1977 [00:02<00:45, 41.78it/s]  5%|▍         | 95/1977 [00:02<00:37, 49.85it/s]  5%|▌         | 105/1977 [00:02<00:30, 60.90it/s]  6%|▌         | 113/1977 [00:02<00:28, 65.59it/s]  7%|▋         | 132/1977 [00:02<00:19, 96.25it/s]  7%|▋         | 143/1977 [00:03<01:04, 28.61it/s]  8%|▊         | 151/1977 [00:04<01:29, 20.32it/s]  8%|▊         | 159/1977 [00:04<01:12, 24.93it/s]  8%|▊         | 167/1977 [00:04<00:59, 30.25it/s]  9%|▉         | 175/1977 [00:04<00:50, 35.95it/s]  9%|▉         | 182/1977 [00:05<00:46, 38.43it/s] 10%|▉         | 189/1977 [00:05<01:11, 24.90it/s] 10%|▉         | 194/1977 [00:05<01:09, 25.75it/s] 11%|█         | 208/1977 [00:05<00:44, 39.34it/s] 11%|█         | 215/1977 [00:06<00:43, 40.47it/s] 11%|█▏        | 224/1977 [00:06<00:36, 48.64it/s] 12%|█▏        | 233/1977 [00:06<00:31, 55.91it/s] 12%|█▏        | 242/1977 [00:06<00:27, 62.00it/s] 13%|█▎        | 250/1977 [00:06<00:28, 59.66it/s] 13%|█▎        | 262/1977 [00:06<00:23, 73.07it/s] 14%|█▍        | 274/1977 [00:06<00:21, 78.84it/s] 14%|█▍        | 283/1977 [00:07<00:32, 51.78it/s] 15%|█▍        | 290/1977 [00:07<00:49, 34.38it/s] 15%|█▍        | 296/1977 [00:07<00:58, 28.97it/s] 15%|█▌        | 301/1977 [00:08<01:03, 26.54it/s] 15%|█▌        | 305/1977 [00:08<01:12, 22.99it/s] 16%|█▌        | 308/1977 [00:08<01:09, 23.87it/s] 16%|█▌        | 312/1977 [00:08<01:03, 26.15it/s] 16%|█▌        | 317/1977 [00:08<00:56, 29.43it/s] 16%|█▋        | 322/1977 [00:08<00:50, 32.57it/s] 17%|█▋        | 327/1977 [00:08<00:46, 35.26it/s] 17%|█▋        | 332/1977 [00:09<00:43, 37.63it/s] 17%|█▋        | 337/1977 [00:09<00:41, 39.47it/s] 17%|█▋        | 342/1977 [00:09<00:40, 40.62it/s] 18%|█▊        | 347/1977 [00:09<00:37, 43.04it/s] 18%|█▊        | 353/1977 [00:09<00:34, 46.54it/s] 18%|█▊        | 358/1977 [00:09<00:34, 46.39it/s] 18%|█▊        | 363/1977 [00:09<00:37, 43.14it/s] 19%|█▊        | 368/1977 [00:09<00:36, 44.01it/s] 19%|█▉        | 384/1977 [00:09<00:21, 74.75it/s] 20%|█▉        | 392/1977 [00:10<00:21, 72.68it/s] 20%|██        | 404/1977 [00:10<00:19, 81.02it/s] 21%|██        | 413/1977 [00:10<00:32, 48.37it/s] 21%|██▏       | 422/1977 [00:10<00:28, 55.46it/s] 22%|██▏       | 430/1977 [00:10<00:37, 41.26it/s] 22%|██▏       | 436/1977 [00:11<00:43, 35.15it/s] 22%|██▏       | 441/1977 [00:11<00:41, 37.06it/s] 23%|██▎       | 446/1977 [00:11<00:39, 39.04it/s] 23%|██▎       | 451/1977 [00:11<00:47, 32.37it/s] 23%|██▎       | 455/1977 [00:11<00:52, 29.00it/s] 23%|██▎       | 459/1977 [00:12<00:55, 27.19it/s] 23%|██▎       | 463/1977 [00:12<00:55, 27.32it/s] 24%|██▍       | 470/1977 [00:12<00:43, 34.57it/s] 24%|██▍       | 474/1977 [00:12<00:47, 31.69it/s] 24%|██▍       | 480/1977 [00:12<00:40, 37.39it/s] 25%|██▍       | 486/1977 [00:12<00:36, 41.28it/s] 25%|██▍       | 494/1977 [00:12<00:29, 50.66it/s] 26%|██▌       | 505/1977 [00:12<00:22, 65.56it/s] 26%|██▌       | 513/1977 [00:12<00:22, 64.84it/s] 27%|██▋       | 532/1977 [00:13<00:14, 97.07it/s] 28%|██▊       | 547/1977 [00:13<00:13, 109.37it/s] 28%|██▊       | 559/1977 [00:13<00:15, 93.08it/s]  29%|██▉       | 570/1977 [00:13<00:16, 85.71it/s] 29%|██▉       | 583/1977 [00:13<00:14, 95.77it/s] 30%|███       | 594/1977 [00:13<00:22, 62.74it/s] 31%|███       | 606/1977 [00:14<00:19, 72.07it/s] 31%|███       | 616/1977 [00:14<00:18, 72.35it/s] 32%|███▏      | 625/1977 [00:14<00:37, 36.01it/s] 32%|███▏      | 632/1977 [00:15<00:55, 24.40it/s] 33%|███▎      | 644/1977 [00:15<00:39, 33.69it/s] 33%|███▎      | 651/1977 [00:15<00:35, 37.35it/s] 33%|███▎      | 659/1977 [00:15<00:30, 43.33it/s] 34%|███▎      | 667/1977 [00:15<00:26, 48.98it/s] 34%|███▍      | 675/1977 [00:15<00:23, 54.49it/s] 35%|███▌      | 692/1977 [00:16<00:16, 79.08it/s] 36%|███▌      | 709/1977 [00:16<00:12, 98.19it/s] 37%|███▋      | 723/1977 [00:16<00:11, 107.32it/s] 37%|███▋      | 736/1977 [00:16<00:12, 102.51it/s] 38%|███▊      | 748/1977 [00:16<00:14, 82.20it/s]  38%|███▊      | 758/1977 [00:16<00:15, 81.20it/s] 39%|███▉      | 768/1977 [00:16<00:15, 77.40it/s] 39%|███▉      | 777/1977 [00:17<00:17, 69.64it/s] 40%|███▉      | 785/1977 [00:17<00:17, 69.94it/s] 40%|████      | 793/1977 [00:17<00:16, 71.35it/s] 41%|████      | 801/1977 [00:17<00:16, 70.95it/s] 41%|████      | 809/1977 [00:17<00:17, 65.43it/s] 41%|████▏     | 816/1977 [00:17<00:17, 64.70it/s] 42%|████▏     | 823/1977 [00:17<00:17, 64.66it/s] 42%|████▏     | 830/1977 [00:17<00:17, 64.79it/s] 42%|████▏     | 837/1977 [00:18<00:17, 65.27it/s] 43%|████▎     | 844/1977 [00:18<00:17, 64.42it/s] 43%|████▎     | 852/1977 [00:18<00:16, 67.12it/s] 43%|████▎     | 859/1977 [00:18<00:18, 61.68it/s] 44%|████▍     | 866/1977 [00:18<00:17, 62.51it/s] 44%|████▍     | 873/1977 [00:18<00:17, 63.21it/s] 45%|████▍     | 881/1977 [00:18<00:16, 66.62it/s] 45%|████▍     | 888/1977 [00:18<00:16, 64.16it/s] 45%|████▌     | 896/1977 [00:18<00:16, 66.88it/s] 46%|████▌     | 903/1977 [00:19<00:24, 43.55it/s] 46%|████▌     | 909/1977 [00:19<00:30, 35.08it/s] 46%|████▌     | 914/1977 [00:19<00:33, 31.47it/s] 46%|████▋     | 918/1977 [00:19<00:36, 28.81it/s] 47%|████▋     | 922/1977 [00:20<00:38, 27.43it/s] 47%|████▋     | 926/1977 [00:20<00:41, 25.27it/s] 47%|████▋     | 929/1977 [00:20<00:40, 26.03it/s] 47%|████▋     | 932/1977 [00:20<00:40, 25.60it/s] 47%|████▋     | 935/1977 [00:20<00:41, 25.31it/s] 48%|████▊     | 941/1977 [00:20<00:32, 32.11it/s] 48%|████▊     | 949/1977 [00:20<00:24, 41.39it/s] 48%|████▊     | 954/1977 [00:20<00:24, 41.31it/s] 49%|████▊     | 959/1977 [00:21<00:24, 41.04it/s] 49%|████▉     | 966/1977 [00:21<00:20, 48.33it/s] 49%|████▉     | 975/1977 [00:21<00:17, 56.92it/s] 50%|████▉     | 981/1977 [00:21<00:21, 45.33it/s] 50%|█████     | 989/1977 [00:21<00:20, 47.75it/s] 50%|█████     | 995/1977 [00:22<00:35, 27.72it/s] 51%|█████     | 999/1977 [00:22<00:43, 22.71it/s] 51%|█████     | 1011/1977 [00:22<00:26, 35.80it/s] 51%|█████▏    | 1017/1977 [00:22<00:29, 32.74it/s] 52%|█████▏    | 1022/1977 [00:22<00:30, 31.35it/s] 52%|█████▏    | 1030/1977 [00:23<00:24, 38.65it/s] 52%|█████▏    | 1035/1977 [00:23<00:24, 38.75it/s] 53%|█████▎    | 1040/1977 [00:23<00:23, 39.34it/s] 53%|█████▎    | 1045/1977 [00:23<00:23, 39.76it/s] 53%|█████▎    | 1050/1977 [00:23<00:23, 40.01it/s] 53%|█████▎    | 1055/1977 [00:23<00:22, 40.16it/s] 54%|█████▎    | 1061/1977 [00:23<00:21, 43.28it/s] 54%|█████▍    | 1073/1977 [00:23<00:14, 61.18it/s] 55%|█████▍    | 1084/1977 [00:24<00:12, 71.24it/s] 55%|█████▌    | 1092/1977 [00:24<00:13, 67.06it/s] 56%|█████▌    | 1102/1977 [00:24<00:11, 73.64it/s] 56%|█████▌    | 1110/1977 [00:24<00:23, 36.92it/s] 56%|█████▋    | 1116/1977 [00:25<00:31, 27.43it/s] 57%|█████▋    | 1121/1977 [00:25<00:35, 23.83it/s] 57%|█████▋    | 1125/1977 [00:25<00:37, 22.92it/s] 57%|█████▋    | 1131/1977 [00:25<00:31, 26.83it/s] 57%|█████▋    | 1135/1977 [00:25<00:31, 26.80it/s] 58%|█████▊    | 1144/1977 [00:26<00:22, 36.54it/s] 58%|█████▊    | 1149/1977 [00:26<00:27, 30.45it/s] 58%|█████▊    | 1153/1977 [00:26<00:30, 27.35it/s] 59%|█████▊    | 1157/1977 [00:26<00:43, 18.84it/s] 59%|█████▊    | 1160/1977 [00:27<00:49, 16.60it/s] 59%|█████▉    | 1163/1977 [00:27<00:49, 16.56it/s] 59%|█████▉    | 1165/1977 [00:27<00:48, 16.72it/s] 59%|█████▉    | 1167/1977 [00:27<00:47, 17.06it/s] 59%|█████▉    | 1169/1977 [00:27<00:46, 17.35it/s] 59%|█████▉    | 1171/1977 [00:27<00:45, 17.63it/s] 59%|█████▉    | 1173/1977 [00:27<00:45, 17.83it/s] 59%|█████▉    | 1175/1977 [00:28<00:59, 13.49it/s] 60%|█████▉    | 1180/1977 [00:28<00:38, 20.67it/s] 60%|█████▉    | 1185/1977 [00:28<00:29, 26.43it/s] 60%|██████    | 1189/1977 [00:28<00:32, 24.43it/s] 60%|██████    | 1192/1977 [00:28<00:33, 23.47it/s] 60%|██████    | 1196/1977 [00:28<00:29, 26.29it/s] 61%|██████    | 1202/1977 [00:28<00:23, 32.29it/s] 61%|██████    | 1206/1977 [00:29<00:23, 33.24it/s] 61%|██████▏   | 1215/1977 [00:29<00:16, 47.23it/s] 62%|██████▏   | 1229/1977 [00:29<00:10, 70.63it/s] 63%|██████▎   | 1237/1977 [00:29<00:14, 51.20it/s] 63%|██████▎   | 1244/1977 [00:29<00:18, 39.39it/s] 64%|██████▎   | 1258/1977 [00:29<00:12, 56.89it/s] 64%|██████▍   | 1266/1977 [00:30<00:16, 43.91it/s] 64%|██████▍   | 1273/1977 [00:30<00:16, 43.31it/s] 65%|██████▍   | 1281/1977 [00:30<00:14, 48.79it/s] 65%|██████▌   | 1288/1977 [00:30<00:14, 48.78it/s] 65%|██████▌   | 1294/1977 [00:30<00:14, 48.66it/s] 66%|██████▌   | 1305/1977 [00:30<00:10, 61.85it/s] 66%|██████▋   | 1314/1977 [00:31<00:09, 66.61it/s] 67%|██████▋   | 1322/1977 [00:31<00:09, 67.85it/s] 67%|██████▋   | 1330/1977 [00:31<00:09, 65.80it/s] 68%|██████▊   | 1337/1977 [00:31<00:11, 53.70it/s] 68%|██████▊   | 1343/1977 [00:31<00:13, 48.52it/s] 68%|██████▊   | 1349/1977 [00:31<00:14, 42.42it/s] 69%|██████▊   | 1355/1977 [00:31<00:13, 44.44it/s] 69%|██████▉   | 1362/1977 [00:32<00:12, 48.50it/s] 69%|██████▉   | 1370/1977 [00:32<00:11, 55.01it/s] 70%|██████▉   | 1376/1977 [00:32<00:12, 48.09it/s] 70%|██████▉   | 1382/1977 [00:32<00:14, 41.74it/s] 70%|███████   | 1387/1977 [00:32<00:15, 37.71it/s] 70%|███████   | 1392/1977 [00:32<00:20, 28.48it/s] 71%|███████   | 1396/1977 [00:33<00:24, 23.66it/s] 71%|███████   | 1399/1977 [00:33<00:27, 21.29it/s] 71%|███████   | 1402/1977 [00:33<00:28, 20.37it/s] 71%|███████   | 1405/1977 [00:33<00:26, 21.45it/s] 71%|███████▏  | 1411/1977 [00:33<00:20, 28.10it/s] 72%|███████▏  | 1417/1977 [00:33<00:16, 33.82it/s] 72%|███████▏  | 1423/1977 [00:34<00:14, 38.48it/s] 72%|███████▏  | 1428/1977 [00:34<00:13, 40.27it/s] 72%|███████▏  | 1433/1977 [00:34<00:13, 41.44it/s] 73%|███████▎  | 1439/1977 [00:34<00:11, 45.58it/s] 73%|███████▎  | 1444/1977 [00:34<00:13, 40.39it/s] 73%|███████▎  | 1449/1977 [00:34<00:22, 23.88it/s] 73%|███████▎  | 1453/1977 [00:35<00:34, 15.10it/s] 74%|███████▎  | 1456/1977 [00:35<00:37, 13.75it/s] 74%|███████▍  | 1460/1977 [00:35<00:31, 16.48it/s] 74%|███████▍  | 1464/1977 [00:36<00:26, 19.42it/s] 74%|███████▍  | 1469/1977 [00:36<00:20, 24.31it/s] 75%|███████▍  | 1473/1977 [00:36<00:19, 26.19it/s] 75%|███████▍  | 1478/1977 [00:36<00:16, 30.05it/s] 75%|███████▍  | 1482/1977 [00:36<00:17, 28.69it/s] 75%|███████▌  | 1486/1977 [00:36<00:16, 29.18it/s] 76%|███████▌  | 1495/1977 [00:36<00:11, 42.94it/s] 76%|███████▌  | 1501/1977 [00:36<00:10, 46.78it/s] 76%|███████▋  | 1509/1977 [00:37<00:08, 53.90it/s] 77%|███████▋  | 1516/1977 [00:37<00:08, 55.20it/s] 77%|███████▋  | 1522/1977 [00:37<00:09, 48.78it/s] 77%|███████▋  | 1528/1977 [00:37<00:11, 38.22it/s] 78%|███████▊  | 1533/1977 [00:37<00:14, 29.91it/s] 78%|███████▊  | 1537/1977 [00:37<00:15, 28.85it/s] 78%|███████▊  | 1541/1977 [00:38<00:16, 27.11it/s] 78%|███████▊  | 1544/1977 [00:38<00:19, 21.87it/s] 78%|███████▊  | 1547/1977 [00:38<00:22, 19.11it/s] 78%|███████▊  | 1550/1977 [00:38<00:22, 18.67it/s] 79%|███████▉  | 1559/1977 [00:38<00:13, 31.05it/s] 80%|███████▉  | 1577/1977 [00:38<00:06, 60.66it/s] 80%|████████  | 1585/1977 [00:39<00:06, 60.45it/s] 81%|████████  | 1593/1977 [00:39<00:06, 60.50it/s] 81%|████████  | 1600/1977 [00:39<00:07, 50.11it/s] 81%|████████▏ | 1607/1977 [00:39<00:06, 53.29it/s] 82%|████████▏ | 1614/1977 [00:39<00:06, 54.21it/s] 82%|████████▏ | 1624/1977 [00:39<00:05, 63.82it/s] 83%|████████▎ | 1633/1977 [00:39<00:04, 70.26it/s] 83%|████████▎ | 1642/1977 [00:40<00:04, 74.02it/s] 84%|████████▎ | 1652/1977 [00:40<00:04, 79.93it/s] 84%|████████▍ | 1665/1977 [00:40<00:03, 92.57it/s] 85%|████████▍ | 1675/1977 [00:40<00:07, 41.74it/s] 85%|████████▌ | 1683/1977 [00:41<00:11, 25.60it/s] 86%|████████▌ | 1697/1977 [00:41<00:07, 37.22it/s] 86%|████████▋ | 1710/1977 [00:41<00:05, 47.56it/s] 87%|████████▋ | 1719/1977 [00:41<00:04, 53.06it/s] 87%|████████▋ | 1729/1977 [00:41<00:04, 60.60it/s] 88%|████████▊ | 1738/1977 [00:42<00:04, 53.42it/s] 89%|████████▉ | 1756/1977 [00:42<00:02, 75.70it/s] 89%|████████▉ | 1767/1977 [00:42<00:02, 81.64it/s] 90%|████████▉ | 1778/1977 [00:42<00:02, 78.97it/s] 90%|█████████ | 1788/1977 [00:42<00:02, 71.12it/s] 91%|█████████ | 1799/1977 [00:42<00:02, 78.98it/s] 92%|█████████▏| 1809/1977 [00:42<00:02, 79.77it/s] 92%|█████████▏| 1818/1977 [00:43<00:03, 45.59it/s] 92%|█████████▏| 1825/1977 [00:43<00:03, 46.84it/s] 93%|█████████▎| 1834/1977 [00:43<00:02, 54.47it/s] 93%|█████████▎| 1842/1977 [00:43<00:02, 56.75it/s] 94%|█████████▎| 1849/1977 [00:43<00:02, 54.68it/s] 94%|█████████▍| 1858/1977 [00:43<00:01, 61.43it/s] 94%|█████████▍| 1866/1977 [00:44<00:01, 63.58it/s] 95%|█████████▍| 1873/1977 [00:44<00:01, 59.37it/s] 95%|█████████▌| 1883/1977 [00:44<00:01, 68.50it/s] 96%|█████████▌| 1896/1977 [00:44<00:00, 83.60it/s] 96%|█████████▋| 1905/1977 [00:44<00:00, 84.82it/s] 97%|█████████▋| 1914/1977 [00:44<00:00, 80.60it/s] 97%|█████████▋| 1923/1977 [00:44<00:00, 54.97it/s] 98%|█████████▊| 1930/1977 [00:45<00:01, 46.35it/s] 98%|█████████▊| 1938/1977 [00:45<00:00, 50.11it/s] 98%|█████████▊| 1944/1977 [00:45<00:01, 30.54it/s] 99%|█████████▊| 1949/1977 [00:46<00:01, 24.19it/s] 99%|█████████▉| 1954/1977 [00:46<00:00, 27.43it/s] 99%|█████████▉| 1958/1977 [00:46<00:00, 27.70it/s] 99%|█████████▉| 1962/1977 [00:46<00:00, 28.59it/s] 99%|█████████▉| 1966/1977 [00:46<00:00, 30.43it/s]100%|█████████▉| 1971/1977 [00:46<00:00, 34.60it/s]100%|██████████| 1977/1977 [00:46<00:00, 42.35it/s]
  0%|          | 0/31 [00:00<?, ?it/s]  3%|▎         | 1/31 [01:19<39:40, 79.35s/it]  6%|▋         | 2/31 [02:37<37:52, 78.38s/it] 10%|▉         | 3/31 [03:54<36:25, 78.04s/it] 13%|█▎        | 4/31 [05:12<35:00, 77.81s/it] 16%|█▌        | 5/31 [06:30<33:47, 77.98s/it] 19%|█▉        | 6/31 [07:48<32:28, 77.94s/it] 23%|██▎       | 7/31 [09:06<31:09, 77.89s/it] 26%|██▌       | 8/31 [10:24<29:53, 77.99s/it] 29%|██▉       | 9/31 [11:42<28:37, 78.08s/it] 32%|███▏      | 10/31 [13:00<27:19, 78.06s/it] 35%|███▌      | 11/31 [14:16<25:45, 77.30s/it] 39%|███▊      | 12/31 [15:34<24:36, 77.70s/it] 42%|████▏     | 13/31 [16:53<23:22, 77.90s/it] 45%|████▌     | 14/31 [18:11<22:05, 77.97s/it] 48%|████▊     | 15/31 [19:29<20:50, 78.18s/it] 52%|█████▏    | 16/31 [20:46<19:24, 77.62s/it] 55%|█████▍    | 17/31 [22:04<18:09, 77.84s/it] 58%|█████▊    | 18/31 [23:22<16:53, 77.98s/it] 61%|██████▏   | 19/31 [24:40<15:35, 77.93s/it] 65%|██████▍   | 20/31 [25:57<14:13, 77.57s/it] 68%|██████▊   | 21/31 [27:15<12:56, 77.70s/it] 71%|███████   | 22/31 [28:33<11:40, 77.87s/it] 74%|███████▍  | 23/31 [29:52<10:24, 78.06s/it] 77%|███████▋  | 24/31 [31:10<09:07, 78.18s/it] 81%|████████  | 25/31 [32:28<07:47, 77.98s/it] 84%|████████▍ | 26/31 [33:45<06:29, 77.88s/it] 87%|████████▋ | 27/31 [35:03<05:11, 77.81s/it] 90%|█████████ | 28/31 [36:21<03:53, 77.80s/it] 94%|█████████▎| 29/31 [37:39<02:35, 77.93s/it] 97%|█████████▋| 30/31 [38:57<01:18, 78.06s/it]100%|██████████| 31/31 [40:07<00:00, 75.56s/it]run.py:270: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:270: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:270: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:270: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
100%|██████████| 31/31 [40:07<00:00, 77.66s/it]
06/09/2022 04:13:04 - INFO - __main__ -     bleu-4 = 1.71 
06/09/2022 04:13:04 - INFO - __main__ -     xMatch = 5.0076 
06/09/2022 04:13:04 - INFO - __main__ -     ********************
