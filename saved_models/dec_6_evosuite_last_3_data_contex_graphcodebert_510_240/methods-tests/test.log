05/04/2022 15:14:23 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=10, cal_blue=1, config_name='graphcodebert-base', dev_filename=None, do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=64, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='saved_models/dec_6_evosuite_last_3_data_contex_graphcodebert_510_240/methods-tests//checkpoint-best-bleu/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=510, max_steps=-1, max_target_length=240, model_name_or_path='saved_models/dec_6_evosuite_last_3_data_contex_graphcodebert_510_240/methods-tests//checkpoint-best-bleu/pytorch_model.bin', model_type='roberta', no_cuda=False, num_train_epochs=3, output_dir='saved_models/dec_6_evosuite_last_3_data_contex_graphcodebert_510_240/methods-tests/', seed=42, source_lang='methods', test_filename='dataset/evosuit/Evosuit_small.methods,dataset/evosuit/Evosuit_small.tests', tokenizer_name='graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
05/04/2022 15:14:23 - INFO - __main__ -   1
Some weights of the model checkpoint at saved_models/dec_6_evosuite_last_3_data_contex_graphcodebert_510_240/methods-tests//checkpoint-best-bleu/pytorch_model.bin were not used when initializing RobertaModel: ['decoder.layers.2.linear2.weight', 'decoder.layers.4.norm1.bias', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.out_proj.bias', 'encoder.encoder.layer.7.output.LayerNorm.weight', 'decoder.layers.3.self_attn.in_proj_bias', 'decoder.layers.0.linear1.bias', 'decoder.layers.5.self_attn.in_proj_weight', 'encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'decoder.layers.4.norm1.weight', 'decoder.layers.2.norm1.bias', 'encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'decoder.layers.4.multihead_attn.out_proj.bias', 'decoder.layers.1.norm1.bias', 'encoder.embeddings.position_embeddings.weight', 'encoder.encoder.layer.0.output.dense.weight', 'encoder.encoder.layer.8.attention.output.dense.weight', 'decoder.layers.5.linear1.weight', 'encoder.encoder.layer.4.attention.output.dense.weight', 'encoder.encoder.layer.9.attention.self.key.bias', 'encoder.encoder.layer.0.attention.self.value.bias', 'decoder.layers.1.linear2.weight', 'encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'decoder.layers.3.norm2.weight', 'encoder.encoder.layer.10.output.dense.bias', 'encoder.encoder.layer.1.attention.output.dense.weight', 'decoder.layers.0.linear2.bias', 'decoder.layers.2.linear1.weight', 'decoder.layers.2.norm3.bias', 'encoder.encoder.layer.4.intermediate.dense.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.encoder.layer.8.attention.self.key.bias', 'decoder.layers.1.norm1.weight', 'encoder.encoder.layer.9.attention.self.value.bias', 'encoder.encoder.layer.3.attention.self.key.weight', 'decoder.layers.3.norm3.bias', 'encoder.encoder.layer.0.attention.self.query.weight', 'decoder.layers.5.norm1.bias', 'encoder.encoder.layer.2.attention.self.key.weight', 'decoder.layers.0.norm1.weight', 'encoder.embeddings.word_embeddings.weight', 'encoder.encoder.layer.8.intermediate.dense.weight', 'encoder.encoder.layer.8.output.LayerNorm.bias', 'decoder.layers.3.norm2.bias', 'decoder.layers.4.self_attn.in_proj_weight', 'decoder.layers.3.self_attn.out_proj.weight', 'encoder.encoder.layer.5.attention.self.key.bias', 'decoder.layers.4.self_attn.out_proj.bias', 'encoder.encoder.layer.1.output.dense.bias', 'encoder.encoder.layer.7.intermediate.dense.weight', 'decoder.layers.4.norm2.weight', 'encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.encoder.layer.10.output.LayerNorm.bias', 'decoder.layers.3.self_attn.in_proj_weight', 'decoder.layers.1.self_attn.in_proj_weight', 'decoder.layers.4.linear1.bias', 'decoder.layers.4.self_attn.in_proj_bias', 'encoder.encoder.layer.1.intermediate.dense.bias', 'encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'decoder.layers.0.linear2.weight', 'encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.output.LayerNorm.weight', 'decoder.layers.0.self_attn.out_proj.bias', 'encoder.encoder.layer.11.attention.output.dense.bias', 'encoder.encoder.layer.8.attention.self.query.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'encoder.encoder.layer.1.attention.self.query.bias', 'encoder.encoder.layer.3.intermediate.dense.bias', 'encoder.encoder.layer.3.output.LayerNorm.bias', 'encoder.encoder.layer.7.attention.self.value.weight', 'encoder.encoder.layer.7.intermediate.dense.bias', 'decoder.layers.5.multihead_attn.out_proj.bias', 'dense.weight', 'encoder.encoder.layer.0.attention.self.value.weight', 'encoder.encoder.layer.9.intermediate.dense.weight', 'encoder.encoder.layer.8.output.dense.bias', 'encoder.encoder.layer.4.attention.self.value.bias', 'encoder.encoder.layer.1.attention.self.query.weight', 'decoder.layers.2.norm1.weight', 'encoder.pooler.dense.weight', 'encoder.encoder.layer.4.output.dense.bias', 'encoder.encoder.layer.7.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.output.dense.weight', 'decoder.layers.2.multihead_attn.out_proj.weight', 'decoder.layers.0.linear1.weight', 'encoder.encoder.layer.5.intermediate.dense.weight', 'decoder.layers.0.self_attn.in_proj_bias', 'encoder.encoder.layer.3.output.dense.weight', 'encoder.encoder.layer.9.output.LayerNorm.weight', 'decoder.layers.5.linear1.bias', 'encoder.encoder.layer.7.attention.output.dense.bias', 'encoder.encoder.layer.10.output.LayerNorm.weight', 'encoder.encoder.layer.2.output.LayerNorm.bias', 'bias', 'decoder.layers.0.multihead_attn.in_proj_bias', 'decoder.layers.1.linear1.weight', 'encoder.encoder.layer.10.intermediate.dense.weight', 'encoder.encoder.layer.3.attention.self.query.bias', 'encoder.encoder.layer.11.output.dense.bias', 'encoder.encoder.layer.8.attention.self.query.weight', 'encoder.encoder.layer.2.output.dense.bias', 'encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.attention.self.key.bias', 'encoder.embeddings.position_ids', 'encoder.encoder.layer.9.attention.self.key.weight', 'encoder.encoder.layer.6.attention.output.dense.weight', 'decoder.layers.3.linear2.weight', 'encoder.encoder.layer.7.attention.self.query.bias', 'encoder.encoder.layer.11.attention.self.query.bias', 'decoder.layers.2.self_attn.in_proj_bias', 'decoder.layers.4.linear2.bias', 'encoder.encoder.layer.4.attention.output.dense.bias', 'encoder.encoder.layer.5.attention.output.dense.weight', 'encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'decoder.layers.0.norm1.bias', 'encoder.encoder.layer.10.attention.self.query.bias', 'decoder.layers.5.multihead_attn.out_proj.weight', 'encoder.encoder.layer.3.attention.output.dense.bias', 'encoder.encoder.layer.5.attention.self.key.weight', 'decoder.layers.5.linear2.bias', 'encoder.encoder.layer.3.intermediate.dense.weight', 'encoder.encoder.layer.6.output.dense.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'encoder.encoder.layer.8.attention.self.key.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'encoder.encoder.layer.11.attention.self.value.bias', 'encoder.encoder.layer.1.attention.self.key.bias', 'encoder.encoder.layer.0.attention.self.key.bias', 'encoder.encoder.layer.7.attention.self.key.weight', 'encoder.pooler.dense.bias', 'decoder.layers.5.self_attn.in_proj_bias', 'encoder.encoder.layer.6.attention.self.query.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'lm_head.weight', 'encoder.encoder.layer.11.attention.self.value.weight', 'encoder.encoder.layer.6.output.LayerNorm.weight', 'encoder.encoder.layer.0.output.LayerNorm.bias', 'decoder.layers.3.linear2.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'encoder.encoder.layer.6.attention.self.key.bias', 'encoder.encoder.layer.1.output.LayerNorm.weight', 'encoder.encoder.layer.10.attention.output.dense.bias', 'decoder.layers.5.norm3.weight', 'encoder.encoder.layer.9.attention.self.query.weight', 'decoder.layers.0.multihead_attn.out_proj.bias', 'encoder.encoder.layer.10.attention.self.key.bias', 'encoder.encoder.layer.2.output.dense.weight', 'encoder.encoder.layer.4.attention.self.value.weight', 'decoder.layers.1.self_attn.in_proj_bias', 'encoder.encoder.layer.11.intermediate.dense.weight', 'encoder.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.6.attention.self.value.weight', 'encoder.encoder.layer.5.output.LayerNorm.bias', 'decoder.layers.3.norm1.bias', 'encoder.encoder.layer.6.attention.self.key.weight', 'decoder.layers.1.linear1.bias', 'encoder.encoder.layer.9.attention.self.query.bias', 'encoder.encoder.layer.8.intermediate.dense.bias', 'decoder.layers.1.norm3.weight', 'encoder.encoder.layer.3.output.LayerNorm.weight', 'encoder.encoder.layer.5.output.LayerNorm.weight', 'encoder.encoder.layer.10.intermediate.dense.bias', 'encoder.encoder.layer.11.attention.self.key.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'encoder.encoder.layer.3.attention.self.value.bias', 'encoder.encoder.layer.8.attention.self.value.bias', 'encoder.encoder.layer.5.output.dense.weight', 'decoder.layers.1.norm3.bias', 'decoder.layers.3.multihead_attn.out_proj.bias', 'decoder.layers.0.self_attn.in_proj_weight', 'decoder.layers.1.multihead_attn.out_proj.weight', 'decoder.layers.2.norm3.weight', 'encoder.encoder.layer.4.intermediate.dense.bias', 'decoder.layers.0.norm3.weight', 'decoder.layers.3.linear1.bias', 'encoder.encoder.layer.5.attention.output.dense.bias', 'encoder.encoder.layer.2.attention.self.query.weight', 'encoder.encoder.layer.9.output.dense.bias', 'encoder.encoder.layer.5.attention.self.query.bias', 'encoder.encoder.layer.5.attention.self.value.weight', 'decoder.layers.0.norm2.weight', 'encoder.encoder.layer.0.attention.output.dense.weight', 'encoder.encoder.layer.1.attention.self.key.weight', 'encoder.encoder.layer.9.attention.self.value.weight', 'decoder.layers.4.multihead_attn.out_proj.weight', 'encoder.encoder.layer.1.attention.self.value.weight', 'encoder.encoder.layer.11.output.dense.weight', 'encoder.encoder.layer.11.output.LayerNorm.weight', 'decoder.layers.4.norm3.weight', 'decoder.layers.3.norm1.weight', 'encoder.encoder.layer.9.output.dense.weight', 'encoder.encoder.layer.10.attention.self.key.weight', 'encoder.encoder.layer.1.intermediate.dense.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.encoder.layer.0.attention.self.query.bias', 'encoder.encoder.layer.2.attention.output.dense.bias', 'encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'decoder.layers.3.norm3.weight', 'encoder.encoder.layer.6.attention.self.value.bias', 'encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.encoder.layer.10.output.dense.weight', 'encoder.encoder.layer.5.intermediate.dense.bias', 'encoder.embeddings.LayerNorm.weight', 'encoder.encoder.layer.0.attention.self.key.weight', 'encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.encoder.layer.7.attention.self.value.bias', 'decoder.layers.4.norm3.bias', 'decoder.layers.4.norm2.bias', 'decoder.layers.2.multihead_attn.in_proj_bias', 'encoder.encoder.layer.2.output.LayerNorm.weight', 'decoder.layers.5.norm1.weight', 'decoder.layers.5.multihead_attn.in_proj_bias', 'decoder.layers.1.linear2.bias', 'encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.encoder.layer.3.output.dense.bias', 'encoder.encoder.layer.3.attention.self.key.bias', 'decoder.layers.5.linear2.weight', 'encoder.encoder.layer.1.output.LayerNorm.bias', 'encoder.encoder.layer.11.intermediate.dense.bias', 'encoder.encoder.layer.2.intermediate.dense.weight', 'encoder.encoder.layer.2.attention.self.query.bias', 'encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.encoder.layer.7.attention.self.query.weight', 'encoder.encoder.layer.8.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.self.value.bias', 'encoder.encoder.layer.2.attention.self.key.bias', 'encoder.encoder.layer.9.output.LayerNorm.bias', 'encoder.encoder.layer.0.output.LayerNorm.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.encoder.layer.1.output.dense.weight', 'decoder.layers.2.self_attn.in_proj_weight', 'encoder.encoder.layer.5.output.dense.bias', 'encoder.encoder.layer.10.attention.output.dense.weight', 'encoder.encoder.layer.9.intermediate.dense.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'encoder.encoder.layer.9.attention.output.dense.bias', 'encoder.encoder.layer.10.attention.self.value.weight', 'decoder.layers.2.linear1.bias', 'decoder.layers.2.norm2.bias', 'encoder.encoder.layer.2.attention.self.value.bias', 'dense.bias', 'decoder.layers.4.multihead_attn.in_proj_weight', 'encoder.encoder.layer.2.attention.output.dense.weight', 'decoder.layers.1.multihead_attn.in_proj_weight', 'encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.encoder.layer.6.intermediate.dense.weight', 'encoder.encoder.layer.7.output.dense.weight', 'decoder.layers.1.multihead_attn.in_proj_bias', 'decoder.layers.3.multihead_attn.in_proj_weight', 'encoder.encoder.layer.6.intermediate.dense.bias', 'encoder.encoder.layer.11.attention.self.query.weight', 'encoder.encoder.layer.10.attention.self.query.weight', 'decoder.layers.4.multihead_attn.in_proj_bias', 'decoder.layers.4.linear1.weight', 'decoder.layers.5.norm2.bias', 'encoder.encoder.layer.11.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.attention.self.query.weight', 'decoder.layers.0.norm3.bias', 'decoder.layers.5.multihead_attn.in_proj_weight', 'encoder.encoder.layer.1.attention.self.value.bias', 'encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.encoder.layer.3.attention.self.query.weight', 'encoder.encoder.layer.4.attention.self.key.weight', 'encoder.encoder.layer.6.attention.self.query.bias', 'decoder.layers.5.norm3.bias', 'encoder.encoder.layer.8.output.dense.weight', 'encoder.encoder.layer.9.attention.output.dense.weight', 'encoder.encoder.layer.2.intermediate.dense.bias', 'decoder.layers.5.norm2.weight', 'decoder.layers.2.multihead_attn.out_proj.bias', 'decoder.layers.0.multihead_attn.in_proj_weight', 'encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'decoder.layers.1.norm2.bias', 'decoder.layers.4.linear2.weight', 'decoder.layers.0.norm2.bias', 'encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.output.LayerNorm.bias', 'decoder.layers.3.multihead_attn.in_proj_bias', 'encoder.encoder.layer.0.output.dense.bias', 'decoder.layers.3.linear1.weight', 'decoder.layers.1.multihead_attn.out_proj.bias', 'encoder.encoder.layer.6.output.LayerNorm.bias', 'encoder.encoder.layer.0.intermediate.dense.bias', 'encoder.encoder.layer.1.attention.output.dense.bias', 'encoder.encoder.layer.6.attention.output.dense.bias', 'decoder.layers.0.multihead_attn.out_proj.weight', 'encoder.encoder.layer.3.attention.output.dense.weight', 'encoder.encoder.layer.11.attention.self.key.weight', 'decoder.layers.2.multihead_attn.in_proj_weight', 'decoder.layers.1.norm2.weight', 'encoder.encoder.layer.0.attention.output.dense.bias', 'decoder.layers.3.multihead_attn.out_proj.weight', 'encoder.encoder.layer.3.attention.self.value.weight', 'encoder.embeddings.token_type_embeddings.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'encoder.encoder.layer.7.attention.self.key.bias', 'encoder.encoder.layer.10.attention.self.value.bias', 'encoder.encoder.layer.7.output.dense.bias', 'encoder.encoder.layer.8.attention.self.value.weight', 'decoder.layers.2.linear2.bias', 'encoder.encoder.layer.11.attention.output.dense.weight', 'encoder.encoder.layer.0.intermediate.dense.weight', 'encoder.encoder.layer.4.attention.self.query.bias', 'decoder.layers.2.norm2.weight', 'encoder.embeddings.LayerNorm.bias', 'encoder.encoder.layer.6.output.dense.bias', 'encoder.encoder.layer.5.attention.self.query.weight', 'encoder.encoder.layer.7.attention.output.dense.weight', 'encoder.encoder.layer.8.attention.output.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at saved_models/dec_6_evosuite_last_3_data_contex_graphcodebert_510_240/methods-tests//checkpoint-best-bleu/pytorch_model.bin and are newly initialized: ['encoder.layer.11.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.8.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/04/2022 15:14:33 - INFO - __main__ -   reload model from saved_models/dec_6_evosuite_last_3_data_contex_graphcodebert_510_240/methods-tests//checkpoint-best-bleu/pytorch_model.bin
05/04/2022 15:14:39 - INFO - __main__ -   Test file: dataset/evosuit/Evosuit_small.methods,dataset/evosuit/Evosuit_small.tests
  0%|          | 0/466 [00:00<?, ?it/s]  0%|          | 2/466 [00:00<02:22,  3.25it/s]  1%|          | 3/466 [00:01<03:18,  2.33it/s]  1%|          | 4/466 [00:01<03:51,  1.99it/s]  1%|          | 5/466 [00:02<04:06,  1.87it/s]  1%|▏         | 6/466 [00:03<04:18,  1.78it/s]  2%|▏         | 7/466 [00:03<04:26,  1.72it/s]  2%|▏         | 8/466 [00:04<04:22,  1.75it/s]  2%|▏         | 9/466 [00:04<04:28,  1.70it/s]  2%|▏         | 10/466 [00:05<04:23,  1.73it/s]  2%|▏         | 11/466 [00:05<04:07,  1.84it/s]  3%|▎         | 12/466 [00:06<03:53,  1.94it/s]  3%|▎         | 13/466 [00:06<03:47,  1.99it/s]  3%|▎         | 14/466 [00:07<03:39,  2.06it/s]  3%|▎         | 15/466 [00:07<03:36,  2.08it/s]  3%|▎         | 16/466 [00:08<03:31,  2.13it/s]  4%|▎         | 17/466 [00:08<03:31,  2.13it/s]  4%|▍         | 18/466 [00:09<03:27,  2.16it/s]  4%|▍         | 19/466 [00:09<03:32,  2.10it/s]  4%|▍         | 20/466 [00:10<03:57,  1.88it/s]  5%|▍         | 21/466 [00:10<03:54,  1.90it/s]  5%|▍         | 22/466 [00:11<03:31,  2.10it/s]  5%|▍         | 23/466 [00:11<03:34,  2.06it/s]  5%|▌         | 24/466 [00:12<03:22,  2.18it/s]  5%|▌         | 25/466 [00:12<03:24,  2.15it/s]  6%|▌         | 26/466 [00:12<03:18,  2.21it/s]  6%|▌         | 27/466 [00:13<03:21,  2.18it/s]  6%|▌         | 28/466 [00:13<03:16,  2.23it/s]  6%|▌         | 29/466 [00:14<03:19,  2.19it/s]  6%|▋         | 30/466 [00:14<03:14,  2.24it/s]  7%|▋         | 31/466 [00:15<03:18,  2.19it/s]  7%|▋         | 32/466 [00:15<03:13,  2.24it/s]  7%|▋         | 33/466 [00:16<03:17,  2.19it/s]  7%|▋         | 34/466 [00:16<03:35,  2.00it/s]  8%|▊         | 35/466 [00:17<03:41,  1.95it/s]  8%|▊         | 36/466 [00:17<03:39,  1.96it/s]  8%|▊         | 37/466 [00:18<03:47,  1.89it/s]  8%|▊         | 38/466 [00:18<03:43,  1.92it/s]  8%|▊         | 39/466 [00:19<03:52,  1.84it/s]  9%|▊         | 40/466 [00:19<03:43,  1.91it/s]  9%|▉         | 41/466 [00:20<03:54,  1.81it/s]  9%|▉         | 42/466 [00:21<04:01,  1.76it/s]  9%|▉         | 43/466 [00:21<04:17,  1.65it/s]  9%|▉         | 44/466 [00:22<03:59,  1.76it/s] 10%|▉         | 45/466 [00:22<04:03,  1.73it/s] 10%|▉         | 46/466 [00:23<03:50,  1.82it/s] 10%|█         | 47/466 [00:23<03:43,  1.87it/s] 10%|█         | 48/466 [00:24<04:04,  1.71it/s] 11%|█         | 49/466 [00:25<04:05,  1.70it/s] 11%|█         | 50/466 [00:25<04:09,  1.67it/s] 11%|█         | 51/466 [00:26<04:08,  1.67it/s] 11%|█         | 52/466 [00:27<04:10,  1.65it/s] 11%|█▏        | 53/466 [00:27<04:12,  1.64it/s] 12%|█▏        | 54/466 [00:28<04:09,  1.65it/s] 12%|█▏        | 55/466 [00:28<04:10,  1.64it/s] 12%|█▏        | 56/466 [00:29<04:11,  1.63it/s] 12%|█▏        | 57/466 [00:30<04:09,  1.64it/s] 12%|█▏        | 58/466 [00:30<04:24,  1.54it/s] 13%|█▎        | 59/466 [00:31<04:17,  1.58it/s] 13%|█▎        | 60/466 [00:32<04:15,  1.59it/s] 13%|█▎        | 61/466 [00:32<03:42,  1.82it/s] 13%|█▎        | 62/466 [00:32<03:21,  2.01it/s] 14%|█▎        | 63/466 [00:33<03:27,  1.94it/s] 14%|█▎        | 64/466 [00:33<03:35,  1.87it/s] 14%|█▍        | 65/466 [00:34<03:41,  1.81it/s] 14%|█▍        | 66/466 [00:35<03:41,  1.80it/s] 14%|█▍        | 67/466 [00:35<03:44,  1.78it/s] 15%|█▍        | 68/466 [00:36<03:42,  1.79it/s] 15%|█▍        | 69/466 [00:36<03:44,  1.77it/s] 15%|█▌        | 70/466 [00:37<03:42,  1.78it/s] 15%|█▌        | 71/466 [00:37<03:44,  1.76it/s] 15%|█▌        | 72/466 [00:38<03:45,  1.75it/s] 16%|█▌        | 73/466 [00:39<03:42,  1.76it/s] 16%|█▌        | 74/466 [00:39<03:44,  1.75it/s] 16%|█▌        | 75/466 [00:40<03:41,  1.76it/s] 16%|█▋        | 76/466 [00:40<03:42,  1.75it/s] 17%|█▋        | 77/466 [00:41<03:40,  1.77it/s] 17%|█▋        | 78/466 [00:41<03:41,  1.75it/s] 17%|█▋        | 79/466 [00:42<03:42,  1.74it/s] 17%|█▋        | 80/466 [00:43<03:39,  1.76it/s] 17%|█▋        | 81/466 [00:43<03:20,  1.92it/s] 18%|█▊        | 82/466 [00:43<03:03,  2.09it/s] 18%|█▊        | 83/466 [00:44<02:52,  2.22it/s] 18%|█▊        | 84/466 [00:44<02:54,  2.19it/s] 18%|█▊        | 85/466 [00:45<03:30,  1.81it/s] 18%|█▊        | 86/466 [00:46<03:52,  1.64it/s] 19%|█▊        | 87/466 [00:46<04:11,  1.51it/s] 19%|█▉        | 88/466 [00:47<04:23,  1.44it/s] 19%|█▉        | 89/466 [00:48<04:31,  1.39it/s] 19%|█▉        | 90/466 [00:49<04:37,  1.35it/s] 20%|█▉        | 91/466 [00:50<04:50,  1.29it/s] 20%|█▉        | 92/466 [00:50<04:11,  1.48it/s] 20%|█▉        | 93/466 [00:50<03:37,  1.72it/s] 20%|██        | 94/466 [00:51<03:15,  1.90it/s] 20%|██        | 95/466 [00:51<02:57,  2.09it/s] 21%|██        | 96/466 [00:52<02:44,  2.25it/s] 21%|██        | 97/466 [00:52<02:38,  2.33it/s] 21%|██        | 98/466 [00:52<02:30,  2.44it/s] 21%|██        | 99/466 [00:53<02:25,  2.52it/s] 21%|██▏       | 100/466 [00:53<02:24,  2.53it/s] 22%|██▏       | 101/466 [00:54<02:21,  2.58it/s] 22%|██▏       | 102/466 [00:54<02:46,  2.19it/s] 22%|██▏       | 103/466 [00:55<02:56,  2.06it/s] 22%|██▏       | 104/466 [00:55<03:06,  1.94it/s] 23%|██▎       | 105/466 [00:56<03:13,  1.87it/s] 23%|██▎       | 106/466 [00:56<03:18,  1.81it/s] 23%|██▎       | 107/466 [00:57<03:21,  1.78it/s] 23%|██▎       | 108/466 [00:58<03:24,  1.75it/s] 23%|██▎       | 109/466 [00:58<03:25,  1.74it/s] 24%|██▎       | 110/466 [00:59<03:25,  1.73it/s] 24%|██▍       | 111/466 [00:59<03:27,  1.71it/s] 24%|██▍       | 112/466 [01:00<03:30,  1.68it/s] 24%|██▍       | 113/466 [01:01<03:39,  1.61it/s] 24%|██▍       | 114/466 [01:01<03:49,  1.53it/s] 25%|██▍       | 115/466 [01:02<03:46,  1.55it/s] 25%|██▍       | 116/466 [01:03<03:36,  1.62it/s] 25%|██▌       | 117/466 [01:03<03:32,  1.65it/s] 25%|██▌       | 118/466 [01:04<03:30,  1.65it/s] 26%|██▌       | 119/466 [01:04<03:27,  1.67it/s] 26%|██▌       | 120/466 [01:05<03:22,  1.71it/s] 26%|██▌       | 121/466 [01:05<03:21,  1.71it/s] 26%|██▌       | 122/466 [01:06<03:20,  1.71it/s] 26%|██▋       | 123/466 [01:07<03:17,  1.74it/s] 27%|██▋       | 124/466 [01:07<03:17,  1.73it/s] 27%|██▋       | 125/466 [01:08<03:18,  1.71it/s] 27%|██▋       | 126/466 [01:08<03:18,  1.71it/s] 27%|██▋       | 127/466 [01:09<02:54,  1.94it/s] 27%|██▋       | 128/466 [01:09<02:43,  2.07it/s] 28%|██▊       | 129/466 [01:10<02:32,  2.21it/s] 28%|██▊       | 130/466 [01:10<02:28,  2.26it/s] 28%|██▊       | 131/466 [01:10<02:22,  2.36it/s] 28%|██▊       | 132/466 [01:11<02:17,  2.43it/s] 29%|██▊       | 133/466 [01:11<02:16,  2.44it/s] 29%|██▉       | 134/466 [01:12<02:13,  2.49it/s] 29%|██▉       | 135/466 [01:12<02:11,  2.52it/s] 29%|██▉       | 136/466 [01:12<02:09,  2.54it/s] 29%|██▉       | 137/466 [01:13<02:25,  2.26it/s] 30%|██▉       | 138/466 [01:13<02:38,  2.06it/s] 30%|██▉       | 139/466 [01:14<02:47,  1.95it/s] 30%|███       | 140/466 [01:14<02:44,  1.99it/s] 30%|███       | 141/466 [01:15<02:43,  1.98it/s] 30%|███       | 142/466 [01:16<02:48,  1.92it/s] 31%|███       | 143/466 [01:16<02:54,  1.85it/s] 31%|███       | 144/466 [01:17<02:55,  1.84it/s] 31%|███       | 145/466 [01:17<02:58,  1.80it/s] 31%|███▏      | 146/466 [01:18<02:50,  1.88it/s] 32%|███▏      | 147/466 [01:18<02:39,  2.00it/s] 32%|███▏      | 148/466 [01:19<02:48,  1.89it/s] 32%|███▏      | 149/466 [01:19<02:45,  1.91it/s] 32%|███▏      | 150/466 [01:20<02:33,  2.06it/s] 32%|███▏      | 151/466 [01:20<02:46,  1.89it/s] 33%|███▎      | 152/466 [01:21<02:41,  1.95it/s] 33%|███▎      | 153/466 [01:21<02:32,  2.05it/s] 33%|███▎      | 154/466 [01:22<02:44,  1.89it/s] 33%|███▎      | 155/466 [01:22<02:39,  1.95it/s] 33%|███▎      | 156/466 [01:23<02:28,  2.09it/s] 34%|███▎      | 157/466 [01:23<02:41,  1.91it/s] 34%|███▍      | 158/466 [01:24<02:39,  1.93it/s] 34%|███▍      | 159/466 [01:24<02:27,  2.08it/s] 34%|███▍      | 160/466 [01:25<02:42,  1.88it/s] 35%|███▍      | 161/466 [01:25<02:37,  1.94it/s] 35%|███▍      | 162/466 [01:26<02:26,  2.08it/s] 35%|███▍      | 163/466 [01:26<02:38,  1.91it/s] 35%|███▌      | 164/466 [01:27<02:37,  1.92it/s] 35%|███▌      | 165/466 [01:27<02:25,  2.07it/s] 36%|███▌      | 166/466 [01:28<02:38,  1.90it/s] 36%|███▌      | 167/466 [01:29<02:57,  1.68it/s] 36%|███▌      | 168/466 [01:29<02:49,  1.76it/s] 36%|███▋      | 169/466 [01:30<02:33,  1.93it/s] 36%|███▋      | 170/466 [01:30<02:57,  1.67it/s] 37%|███▋      | 171/466 [01:31<02:49,  1.74it/s] 37%|███▋      | 172/466 [01:31<02:33,  1.92it/s] 37%|███▋      | 173/466 [01:32<02:41,  1.81it/s] 37%|███▋      | 174/466 [01:32<02:34,  1.89it/s] 38%|███▊      | 175/466 [01:33<02:22,  2.04it/s] 38%|███▊      | 176/466 [01:33<02:34,  1.88it/s] 38%|███▊      | 177/466 [01:34<02:31,  1.90it/s] 38%|███▊      | 178/466 [01:34<02:20,  2.05it/s] 38%|███▊      | 179/466 [01:35<02:19,  2.06it/s] 39%|███▊      | 180/466 [01:35<02:13,  2.14it/s] 39%|███▉      | 181/466 [01:36<02:14,  2.13it/s] 39%|███▉      | 182/466 [01:36<02:09,  2.19it/s] 39%|███▉      | 183/466 [01:37<02:36,  1.81it/s] 39%|███▉      | 184/466 [01:38<02:52,  1.63it/s] 40%|███▉      | 185/466 [01:38<03:06,  1.51it/s] 40%|███▉      | 186/466 [01:39<02:50,  1.65it/s] 40%|████      | 187/466 [01:40<03:26,  1.35it/s] 40%|████      | 188/466 [01:41<03:10,  1.46it/s] 41%|████      | 189/466 [01:41<03:17,  1.40it/s] 41%|████      | 190/466 [01:42<02:55,  1.57it/s] 41%|████      | 191/466 [01:43<03:06,  1.47it/s] 41%|████      | 192/466 [01:43<02:49,  1.61it/s] 41%|████▏     | 193/466 [01:44<02:59,  1.52it/s] 42%|████▏     | 194/466 [01:44<02:44,  1.65it/s] 42%|████▏     | 195/466 [01:45<02:58,  1.52it/s] 42%|████▏     | 196/466 [01:45<02:40,  1.68it/s] 42%|████▏     | 197/466 [01:46<02:55,  1.54it/s] 42%|████▏     | 198/466 [01:47<02:38,  1.69it/s] 43%|████▎     | 199/466 [01:47<02:52,  1.55it/s] 43%|████▎     | 200/466 [01:48<02:38,  1.68it/s] 43%|████▎     | 201/466 [01:49<02:52,  1.54it/s] 43%|████▎     | 202/466 [01:49<02:35,  1.69it/s] 44%|████▎     | 203/466 [01:50<02:50,  1.54it/s] 44%|████▍     | 204/466 [01:50<02:34,  1.70it/s] 44%|████▍     | 205/466 [01:51<02:49,  1.54it/s] 44%|████▍     | 206/466 [01:52<02:26,  1.77it/s] 44%|████▍     | 207/466 [01:52<02:13,  1.94it/s] 45%|████▍     | 208/466 [01:52<02:01,  2.13it/s] 45%|████▍     | 209/466 [01:53<01:55,  2.23it/s] 45%|████▌     | 210/466 [01:53<01:48,  2.36it/s] 45%|████▌     | 211/466 [01:53<01:43,  2.45it/s] 45%|████▌     | 212/466 [01:54<01:42,  2.47it/s] 46%|████▌     | 213/466 [01:54<01:39,  2.54it/s] 46%|████▌     | 214/466 [01:55<01:37,  2.59it/s] 46%|████▌     | 215/466 [01:55<01:46,  2.37it/s] 46%|████▋     | 216/466 [01:56<01:59,  2.10it/s] 47%|████▋     | 217/466 [01:56<01:50,  2.25it/s] 47%|████▋     | 218/466 [01:56<01:44,  2.37it/s] 47%|████▋     | 219/466 [01:57<01:42,  2.41it/s] 47%|████▋     | 220/466 [01:57<01:38,  2.50it/s] 47%|████▋     | 221/466 [01:58<01:35,  2.56it/s] 48%|████▊     | 222/466 [01:58<01:35,  2.55it/s] 48%|████▊     | 223/466 [01:58<01:33,  2.60it/s] 48%|████▊     | 224/466 [01:59<01:34,  2.57it/s] 48%|████▊     | 225/466 [01:59<01:32,  2.61it/s] 48%|████▊     | 226/466 [02:00<01:30,  2.64it/s] 49%|████▊     | 227/466 [02:00<01:31,  2.60it/s] 49%|████▉     | 228/466 [02:00<01:30,  2.63it/s] 49%|████▉     | 229/466 [02:01<01:39,  2.39it/s] 49%|████▉     | 230/466 [02:01<01:37,  2.43it/s] 50%|████▉     | 231/466 [02:02<01:58,  1.98it/s] 50%|████▉     | 232/466 [02:02<01:56,  2.01it/s] 50%|█████     | 233/466 [02:03<01:48,  2.14it/s] 50%|█████     | 234/466 [02:03<01:59,  1.94it/s] 50%|█████     | 235/466 [02:04<01:50,  2.08it/s] 51%|█████     | 236/466 [02:04<01:50,  2.09it/s] 51%|█████     | 237/466 [02:05<01:44,  2.20it/s] 51%|█████     | 238/466 [02:05<01:55,  1.97it/s] 51%|█████▏    | 239/466 [02:06<01:45,  2.15it/s] 52%|█████▏    | 240/466 [02:06<01:40,  2.25it/s] 52%|█████▏    | 241/466 [02:06<01:34,  2.37it/s] 52%|█████▏    | 242/466 [02:07<01:32,  2.41it/s] 52%|█████▏    | 243/466 [02:07<01:29,  2.50it/s] 52%|█████▏    | 244/466 [02:08<01:31,  2.43it/s] 53%|█████▎    | 245/466 [02:08<01:35,  2.33it/s] 53%|█████▎    | 246/466 [02:09<01:42,  2.15it/s] 53%|█████▎    | 247/466 [02:09<01:49,  2.00it/s] 53%|█████▎    | 248/466 [02:10<02:03,  1.76it/s] 53%|█████▎    | 249/466 [02:10<01:54,  1.89it/s] 54%|█████▎    | 250/466 [02:11<01:57,  1.84it/s] 54%|█████▍    | 251/466 [02:12<01:57,  1.83it/s] 54%|█████▍    | 252/466 [02:12<01:59,  1.80it/s] 54%|█████▍    | 253/466 [02:13<01:57,  1.81it/s] 55%|█████▍    | 254/466 [02:13<01:59,  1.78it/s] 55%|█████▍    | 255/466 [02:14<01:57,  1.79it/s] 55%|█████▍    | 256/466 [02:14<01:51,  1.88it/s] 55%|█████▌    | 257/466 [02:15<01:45,  1.98it/s] 55%|█████▌    | 258/466 [02:15<01:43,  2.02it/s] 56%|█████▌    | 259/466 [02:16<01:45,  1.96it/s] 56%|█████▌    | 260/466 [02:16<01:49,  1.88it/s] 56%|█████▌    | 261/466 [02:17<01:50,  1.86it/s] 56%|█████▌    | 262/466 [02:17<01:53,  1.80it/s] 56%|█████▋    | 263/466 [02:18<01:47,  1.89it/s] 57%|█████▋    | 264/466 [02:18<01:48,  1.87it/s] 57%|█████▋    | 265/466 [02:19<01:50,  1.82it/s] 57%|█████▋    | 266/466 [02:20<01:49,  1.82it/s] 57%|█████▋    | 267/466 [02:20<01:51,  1.78it/s] 58%|█████▊    | 268/466 [02:21<01:50,  1.79it/s] 58%|█████▊    | 269/466 [02:22<02:05,  1.57it/s] 58%|█████▊    | 270/466 [02:22<02:01,  1.61it/s] 58%|█████▊    | 271/466 [02:23<01:56,  1.67it/s] 58%|█████▊    | 272/466 [02:23<01:55,  1.68it/s] 59%|█████▊    | 273/466 [02:24<01:41,  1.90it/s] 59%|█████▉    | 274/466 [02:24<01:46,  1.79it/s] 59%|█████▉    | 275/466 [02:25<01:48,  1.76it/s] 59%|█████▉    | 276/466 [02:26<01:51,  1.70it/s] 59%|█████▉    | 277/466 [02:26<01:53,  1.67it/s] 60%|█████▉    | 278/466 [02:27<01:50,  1.71it/s] 60%|█████▉    | 279/466 [02:27<01:37,  1.92it/s] 60%|██████    | 280/466 [02:28<01:42,  1.81it/s] 60%|██████    | 281/466 [02:28<01:46,  1.74it/s] 61%|██████    | 282/466 [02:29<01:44,  1.76it/s] 61%|██████    | 283/466 [02:29<01:45,  1.74it/s] 61%|██████    | 284/466 [02:30<01:33,  1.95it/s] 61%|██████    | 285/466 [02:30<01:27,  2.07it/s] 61%|██████▏   | 286/466 [02:31<01:32,  1.94it/s] 62%|██████▏   | 287/466 [02:31<01:38,  1.82it/s] 62%|██████▏   | 288/466 [02:32<01:41,  1.75it/s] 62%|██████▏   | 289/466 [02:33<01:42,  1.72it/s] 62%|██████▏   | 290/466 [02:33<01:44,  1.68it/s] 62%|██████▏   | 291/466 [02:34<01:44,  1.68it/s] 63%|██████▎   | 292/466 [02:35<01:45,  1.65it/s] 63%|██████▎   | 293/466 [02:35<01:32,  1.87it/s] 63%|██████▎   | 294/466 [02:35<01:34,  1.82it/s] 63%|██████▎   | 295/466 [02:36<01:35,  1.78it/s] 64%|██████▎   | 296/466 [02:37<01:35,  1.79it/s] 64%|██████▎   | 297/466 [02:37<01:36,  1.76it/s] 64%|██████▍   | 298/466 [02:38<01:25,  1.97it/s] 64%|██████▍   | 299/466 [02:38<01:30,  1.84it/s] 64%|██████▍   | 300/466 [02:39<01:32,  1.79it/s] 65%|██████▍   | 301/466 [02:39<01:33,  1.76it/s] 65%|██████▍   | 302/466 [02:40<01:25,  1.93it/s] 65%|██████▌   | 303/466 [02:40<01:22,  1.97it/s] 65%|██████▌   | 304/466 [02:41<01:26,  1.87it/s] 65%|██████▌   | 305/466 [02:42<01:30,  1.78it/s] 66%|██████▌   | 306/466 [02:42<01:33,  1.72it/s] 66%|██████▌   | 307/466 [02:43<01:33,  1.71it/s] 66%|██████▌   | 308/466 [02:43<01:23,  1.88it/s] 66%|██████▋   | 309/466 [02:44<01:26,  1.82it/s] 67%|██████▋   | 310/466 [02:44<01:18,  1.98it/s] 67%|██████▋   | 311/466 [02:45<01:22,  1.88it/s] 67%|██████▋   | 312/466 [02:45<01:26,  1.78it/s] 67%|██████▋   | 313/466 [02:46<01:17,  1.99it/s] 67%|██████▋   | 314/466 [02:46<01:10,  2.16it/s] 68%|██████▊   | 315/466 [02:46<01:07,  2.25it/s] 68%|██████▊   | 316/466 [02:47<01:14,  2.00it/s] 68%|██████▊   | 317/466 [02:48<01:18,  1.89it/s] 68%|██████▊   | 318/466 [02:48<01:22,  1.79it/s] 68%|██████▊   | 319/466 [02:49<01:21,  1.80it/s] 69%|██████▊   | 320/466 [02:49<01:14,  1.96it/s] 69%|██████▉   | 321/466 [02:50<01:18,  1.85it/s] 69%|██████▉   | 322/466 [02:51<01:22,  1.74it/s] 69%|██████▉   | 323/466 [02:51<01:24,  1.69it/s] 70%|██████▉   | 324/466 [02:52<01:24,  1.69it/s] 70%|██████▉   | 325/466 [02:52<01:25,  1.66it/s] 70%|██████▉   | 326/466 [02:53<01:19,  1.77it/s] 70%|███████   | 327/466 [02:53<01:13,  1.90it/s] 70%|███████   | 328/466 [02:54<01:15,  1.83it/s] 71%|███████   | 329/466 [02:55<01:16,  1.79it/s] 71%|███████   | 330/466 [02:55<01:11,  1.90it/s] 71%|███████   | 331/466 [02:55<01:09,  1.95it/s] 71%|███████   | 332/466 [02:56<01:06,  2.03it/s] 71%|███████▏  | 333/466 [02:56<01:05,  2.04it/s] 72%|███████▏  | 334/466 [02:57<01:02,  2.10it/s] 72%|███████▏  | 335/466 [02:57<01:02,  2.10it/s] 72%|███████▏  | 336/466 [02:58<01:00,  2.14it/s] 72%|███████▏  | 337/466 [02:58<01:00,  2.12it/s] 73%|███████▎  | 338/466 [02:59<00:56,  2.28it/s] 73%|███████▎  | 339/466 [02:59<00:54,  2.32it/s] 73%|███████▎  | 340/466 [02:59<00:54,  2.30it/s] 73%|███████▎  | 341/466 [03:00<00:53,  2.33it/s] 73%|███████▎  | 342/466 [03:00<00:59,  2.07it/s] 74%|███████▎  | 343/466 [03:01<00:56,  2.16it/s] 74%|███████▍  | 344/466 [03:01<00:55,  2.18it/s] 74%|███████▍  | 345/466 [03:02<00:53,  2.25it/s] 74%|███████▍  | 346/466 [03:02<00:52,  2.30it/s] 74%|███████▍  | 347/466 [03:03<00:52,  2.28it/s] 75%|███████▍  | 348/466 [03:03<00:50,  2.32it/s] 75%|███████▍  | 349/466 [03:03<00:50,  2.30it/s] 75%|███████▌  | 350/466 [03:04<00:49,  2.33it/s] 75%|███████▌  | 351/466 [03:04<00:49,  2.31it/s] 76%|███████▌  | 352/466 [03:05<00:48,  2.34it/s] 76%|███████▌  | 353/466 [03:05<00:47,  2.37it/s] 76%|███████▌  | 354/466 [03:06<00:48,  2.33it/s] 76%|███████▌  | 355/466 [03:06<00:47,  2.36it/s] 76%|███████▋  | 356/466 [03:06<00:47,  2.32it/s] 77%|███████▋  | 357/466 [03:07<00:48,  2.25it/s] 77%|███████▋  | 358/466 [03:07<00:50,  2.16it/s] 77%|███████▋  | 359/466 [03:08<00:49,  2.14it/s] 77%|███████▋  | 360/466 [03:08<00:50,  2.08it/s] 77%|███████▋  | 361/466 [03:09<00:50,  2.08it/s] 78%|███████▊  | 362/466 [03:09<00:51,  2.03it/s] 78%|███████▊  | 363/466 [03:10<00:52,  1.98it/s] 78%|███████▊  | 364/466 [03:10<00:52,  1.96it/s] 78%|███████▊  | 365/466 [03:11<00:50,  2.00it/s] 79%|███████▊  | 366/466 [03:11<00:48,  2.06it/s] 79%|███████▉  | 367/466 [03:12<00:45,  2.15it/s] 79%|███████▉  | 368/466 [03:12<00:44,  2.18it/s] 79%|███████▉  | 369/466 [03:13<00:43,  2.24it/s] 79%|███████▉  | 370/466 [03:13<00:41,  2.29it/s] 80%|███████▉  | 371/466 [03:14<00:41,  2.27it/s] 80%|███████▉  | 372/466 [03:14<00:40,  2.31it/s] 80%|████████  | 373/466 [03:14<00:40,  2.29it/s] 80%|████████  | 374/466 [03:15<00:39,  2.32it/s] 80%|████████  | 375/466 [03:15<00:39,  2.29it/s] 81%|████████  | 376/466 [03:16<00:38,  2.33it/s] 81%|████████  | 377/466 [03:16<00:37,  2.35it/s] 81%|████████  | 378/466 [03:17<00:38,  2.31it/s] 81%|████████▏ | 379/466 [03:17<00:37,  2.34it/s] 82%|████████▏ | 380/466 [03:17<00:37,  2.30it/s] 82%|████████▏ | 381/466 [03:18<00:36,  2.33it/s] 82%|████████▏ | 382/466 [03:18<00:36,  2.30it/s] 82%|████████▏ | 383/466 [03:19<00:35,  2.33it/s] 82%|████████▏ | 384/466 [03:19<00:34,  2.35it/s] 83%|████████▎ | 385/466 [03:20<00:35,  2.31it/s] 83%|████████▎ | 386/466 [03:20<00:34,  2.31it/s] 83%|████████▎ | 387/466 [03:21<00:37,  2.12it/s] 83%|████████▎ | 388/466 [03:21<00:35,  2.20it/s] 83%|████████▎ | 389/466 [03:21<00:34,  2.21it/s] 84%|████████▎ | 390/466 [03:22<00:33,  2.26it/s] 84%|████████▍ | 391/466 [03:22<00:32,  2.30it/s] 84%|████████▍ | 392/466 [03:23<00:33,  2.18it/s] 84%|████████▍ | 393/466 [03:23<00:32,  2.25it/s] 85%|████████▍ | 394/466 [03:24<00:32,  2.24it/s] 85%|████████▍ | 395/466 [03:24<00:31,  2.29it/s] 85%|████████▍ | 396/466 [03:25<00:30,  2.27it/s] 85%|████████▌ | 397/466 [03:25<00:29,  2.31it/s] 85%|████████▌ | 398/466 [03:25<00:29,  2.34it/s] 86%|████████▌ | 399/466 [03:26<00:29,  2.30it/s] 86%|████████▌ | 400/466 [03:26<00:29,  2.23it/s] 86%|████████▌ | 401/466 [03:27<00:28,  2.26it/s] 86%|████████▋ | 402/466 [03:27<00:29,  2.20it/s] 86%|████████▋ | 403/466 [03:28<00:28,  2.24it/s] 87%|████████▋ | 404/466 [03:28<00:30,  2.03it/s] 87%|████████▋ | 405/466 [03:29<00:30,  2.00it/s] 87%|████████▋ | 406/466 [03:29<00:28,  2.13it/s] 87%|████████▋ | 407/466 [03:30<00:30,  1.93it/s] 88%|████████▊ | 408/466 [03:30<00:32,  1.81it/s] 88%|████████▊ | 409/466 [03:31<00:29,  1.93it/s] 88%|████████▊ | 410/466 [03:31<00:26,  2.10it/s] 88%|████████▊ | 411/466 [03:32<00:26,  2.05it/s] 88%|████████▊ | 412/466 [03:32<00:24,  2.17it/s] 89%|████████▊ | 413/466 [03:33<00:24,  2.13it/s] 89%|████████▉ | 414/466 [03:33<00:28,  1.81it/s] 89%|████████▉ | 415/466 [03:34<00:27,  1.84it/s] 89%|████████▉ | 416/466 [03:34<00:24,  2.00it/s] 89%|████████▉ | 417/466 [03:35<00:26,  1.85it/s] 90%|████████▉ | 418/466 [03:35<00:25,  1.92it/s] 90%|████████▉ | 419/466 [03:36<00:23,  2.02it/s] 90%|█████████ | 420/466 [03:36<00:21,  2.17it/s] 90%|█████████ | 421/466 [03:37<00:20,  2.18it/s] 91%|█████████ | 422/466 [03:37<00:19,  2.31it/s] 91%|█████████ | 423/466 [03:37<00:18,  2.28it/s] 91%|█████████ | 424/466 [03:38<00:18,  2.22it/s] 91%|█████████ | 425/466 [03:39<00:20,  2.00it/s] 91%|█████████▏| 426/466 [03:39<00:19,  2.03it/s] 92%|█████████▏| 427/466 [03:39<00:18,  2.10it/s] 92%|█████████▏| 428/466 [03:40<00:20,  1.90it/s] 92%|█████████▏| 429/466 [03:41<00:19,  1.92it/s] 92%|█████████▏| 430/466 [03:41<00:21,  1.66it/s] 92%|█████████▏| 431/466 [03:42<00:22,  1.55it/s] 93%|█████████▎| 432/466 [03:43<00:21,  1.56it/s] 93%|█████████▎| 433/466 [03:43<00:19,  1.68it/s] 93%|█████████▎| 434/466 [03:44<00:17,  1.83it/s] 93%|█████████▎| 435/466 [03:44<00:15,  1.97it/s] 94%|█████████▎| 436/466 [03:45<00:15,  1.96it/s] 94%|█████████▍| 437/466 [03:45<00:15,  1.89it/s] 94%|█████████▍| 438/466 [03:46<00:13,  2.03it/s] 94%|█████████▍| 439/466 [03:46<00:12,  2.19it/s] 94%|█████████▍| 440/466 [03:46<00:11,  2.27it/s] 95%|█████████▍| 441/466 [03:47<00:12,  2.05it/s] 95%|█████████▍| 442/466 [03:48<00:12,  1.93it/s] 95%|█████████▌| 443/466 [03:48<00:11,  1.97it/s] 95%|█████████▌| 444/466 [03:49<00:11,  1.86it/s] 95%|█████████▌| 445/466 [03:49<00:12,  1.63it/s] 96%|█████████▌| 446/466 [03:50<00:10,  1.87it/s] 96%|█████████▌| 447/466 [03:50<00:10,  1.75it/s] 96%|█████████▌| 448/466 [03:51<00:10,  1.73it/s] 96%|█████████▋| 449/466 [03:52<00:10,  1.68it/s] 97%|█████████▋| 450/466 [03:52<00:08,  1.79it/s] 97%|█████████▋| 451/466 [03:53<00:07,  1.91it/s] 97%|█████████▋| 452/466 [03:53<00:06,  2.11it/s] 97%|█████████▋| 453/466 [03:54<00:06,  2.06it/s] 97%|█████████▋| 454/466 [03:54<00:06,  1.95it/s] 98%|█████████▊| 455/466 [03:55<00:06,  1.82it/s] 98%|█████████▊| 456/466 [03:55<00:05,  1.77it/s] 98%|█████████▊| 457/466 [03:56<00:05,  1.71it/s] 98%|█████████▊| 458/466 [03:56<00:04,  1.77it/s] 98%|█████████▊| 459/466 [03:57<00:03,  1.76it/s] 99%|█████████▊| 460/466 [03:58<00:03,  1.81it/s] 99%|█████████▉| 461/466 [03:58<00:02,  1.98it/s] 99%|█████████▉| 462/466 [03:58<00:02,  1.97it/s] 99%|█████████▉| 463/466 [03:59<00:01,  1.89it/s]100%|█████████▉| 464/466 [04:00<00:01,  1.65it/s]100%|█████████▉| 465/466 [04:01<00:00,  1.44it/s]100%|██████████| 466/466 [04:02<00:00,  1.40it/s]100%|██████████| 466/466 [04:02<00:00,  1.93it/s]
  0%|          | 0/8 [00:00<?, ?it/s]/project/6025819/sepehr8/test_generation/Graph_codeBERT_TestGen/model.py:175: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  prevK = bestScoresId // numWords
 12%|█▎        | 1/8 [01:20<09:24, 80.65s/it] 25%|██▌       | 2/8 [02:26<07:11, 71.86s/it] 38%|███▊      | 3/8 [03:34<05:50, 70.02s/it] 50%|█████     | 4/8 [04:51<04:51, 73.00s/it] 62%|██████▎   | 5/8 [06:18<03:53, 77.94s/it] 75%|███████▌  | 6/8 [07:21<02:25, 72.93s/it] 88%|████████▊ | 7/8 [08:26<01:10, 70.13s/it]100%|██████████| 8/8 [08:44<00:00, 53.83s/it]run.py:270: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:270: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:270: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:270: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
100%|██████████| 8/8 [08:45<00:00, 65.63s/it]
05/04/2022 15:27:28 - INFO - __main__ -     bleu-4 = 52.46 
05/04/2022 15:27:28 - INFO - __main__ -     xMatch = 2.5751 
05/04/2022 15:27:28 - INFO - __main__ -     ********************
