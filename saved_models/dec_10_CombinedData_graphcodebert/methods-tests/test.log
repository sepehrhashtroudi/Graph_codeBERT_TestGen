01/12/2022 03:12:37 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=10, config_name='graphcodebert-base', dev_filename='dataset/evosuit/Evosuit_test_lang3_ArrayUtils.methods,dataset/evosuit/Evosuit_test_lang3_ArrayUtils.tests', do_eval=False, do_lower_case=False, do_test=True, do_train=False, eval_batch_size=64, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path='saved_models/dec_10_CombinedData_graphcodebert/methods-tests/checkpoint-best-bleu/pytorch_model.bin', local_rank=-1, max_grad_norm=1.0, max_source_length=320, max_steps=-1, max_target_length=256, model_name_or_path='saved_models/dec_10_CombinedData_graphcodebert/methods-tests/checkpoint-best-bleu/pytorch_model.bin', model_type='roberta', no_cuda=False, num_train_epochs=3, output_dir='saved_models/dec_10_CombinedData_graphcodebert/methods-tests', seed=42, source_lang='methods', test_filename='dataset/evosuit/Evosuit_test_lang3_ArrayUtils.methods,dataset/evosuit/Evosuit_test_lang3_ArrayUtils.tests', tokenizer_name='graphcodebert-base', train_batch_size=8, train_filename=None, train_steps=-1, warmup_steps=0, weight_decay=0.0)
Some weights of the model checkpoint at saved_models/dec_10_CombinedData_graphcodebert/methods-tests/checkpoint-best-bleu/pytorch_model.bin were not used when initializing RobertaModel: ['encoder.encoder.layer.5.output.LayerNorm.bias', 'encoder.encoder.layer.9.attention.output.dense.weight', 'decoder.layers.2.norm1.bias', 'encoder.encoder.layer.0.attention.self.key.weight', 'encoder.encoder.layer.3.attention.self.value.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.2.linear2.bias', 'decoder.layers.3.self_attn.in_proj_weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'decoder.layers.6.norm2.bias', 'encoder.encoder.layer.7.attention.output.dense.weight', 'encoder.encoder.layer.10.output.dense.bias', 'decoder.layers.0.norm1.bias', 'decoder.layers.5.norm1.bias', 'encoder.encoder.layer.5.intermediate.dense.weight', 'encoder.encoder.layer.4.output.LayerNorm.weight', 'encoder.embeddings.word_embeddings.weight', 'encoder.encoder.layer.1.attention.output.dense.bias', 'decoder.layers.5.multihead_attn.out_proj.weight', 'decoder.layers.2.linear2.weight', 'encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'decoder.layers.1.norm3.weight', 'decoder.layers.1.multihead_attn.out_proj.bias', 'decoder.layers.0.multihead_attn.in_proj_bias', 'decoder.layers.6.linear1.bias', 'encoder.encoder.layer.2.attention.self.query.bias', 'encoder.encoder.layer.0.output.LayerNorm.bias', 'encoder.encoder.layer.8.attention.self.value.weight', 'decoder.layers.6.norm1.bias', 'decoder.layers.9.multihead_attn.in_proj_bias', 'decoder.layers.7.self_attn.out_proj.weight', 'encoder.encoder.layer.0.intermediate.dense.bias', 'encoder.encoder.layer.10.attention.output.dense.weight', 'encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'decoder.layers.2.norm2.bias', 'decoder.layers.9.multihead_attn.out_proj.weight', 'decoder.layers.7.norm1.bias', 'encoder.encoder.layer.11.output.LayerNorm.weight', 'decoder.layers.7.self_attn.out_proj.bias', 'encoder.encoder.layer.0.attention.self.key.bias', 'decoder.layers.8.norm1.bias', 'decoder.layers.0.norm2.weight', 'encoder.encoder.layer.2.output.LayerNorm.bias', 'decoder.layers.9.norm1.weight', 'encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.encoder.layer.7.attention.output.dense.bias', 'encoder.encoder.layer.8.output.LayerNorm.bias', 'encoder.encoder.layer.6.output.LayerNorm.bias', 'decoder.layers.5.linear1.bias', 'decoder.layers.1.linear2.weight', 'decoder.layers.1.norm2.weight', 'decoder.layers.4.multihead_attn.in_proj_weight', 'encoder.encoder.layer.2.attention.self.key.bias', 'encoder.encoder.layer.2.output.dense.weight', 'encoder.encoder.layer.1.attention.output.dense.weight', 'encoder.encoder.layer.10.attention.self.value.bias', 'decoder.layers.1.linear1.weight', 'encoder.encoder.layer.5.attention.self.value.weight', 'encoder.encoder.layer.1.attention.self.key.bias', 'decoder.layers.6.linear2.weight', 'encoder.encoder.layer.4.intermediate.dense.bias', 'decoder.layers.0.norm2.bias', 'encoder.encoder.layer.11.attention.self.key.weight', 'encoder.encoder.layer.10.output.LayerNorm.weight', 'decoder.layers.1.self_attn.in_proj_weight', 'decoder.layers.6.linear1.weight', 'encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.encoder.layer.3.output.dense.weight', 'encoder.encoder.layer.6.attention.self.query.bias', 'decoder.layers.8.multihead_attn.out_proj.weight', 'decoder.layers.5.multihead_attn.in_proj_bias', 'decoder.layers.3.norm2.bias', 'decoder.layers.7.multihead_attn.out_proj.weight', 'decoder.layers.2.norm3.bias', 'encoder.encoder.layer.2.attention.self.value.bias', 'decoder.layers.0.multihead_attn.out_proj.bias', 'encoder.encoder.layer.7.output.LayerNorm.weight', 'encoder.encoder.layer.6.attention.output.dense.weight', 'decoder.layers.4.linear1.bias', 'decoder.layers.4.linear2.bias', 'decoder.layers.9.norm2.weight', 'decoder.layers.5.norm3.weight', 'encoder.encoder.layer.11.intermediate.dense.weight', 'decoder.layers.0.linear2.weight', 'encoder.encoder.layer.2.output.LayerNorm.weight', 'encoder.encoder.layer.3.intermediate.dense.weight', 'encoder.encoder.layer.1.attention.self.query.bias', 'encoder.encoder.layer.1.output.dense.weight', 'encoder.encoder.layer.5.attention.self.key.weight', 'encoder.encoder.layer.8.attention.self.key.bias', 'decoder.layers.5.linear2.bias', 'decoder.layers.3.norm3.bias', 'decoder.layers.1.norm3.bias', 'encoder.encoder.layer.9.output.LayerNorm.weight', 'decoder.layers.3.norm2.weight', 'encoder.encoder.layer.2.attention.output.dense.weight', 'decoder.layers.3.norm1.weight', 'encoder.encoder.layer.0.output.dense.bias', 'decoder.layers.2.multihead_attn.out_proj.bias', 'encoder.encoder.layer.0.attention.self.query.bias', 'encoder.encoder.layer.11.attention.self.value.bias', 'encoder.encoder.layer.9.attention.self.key.bias', 'decoder.layers.9.self_attn.in_proj_bias', 'encoder.encoder.layer.10.attention.self.key.weight', 'decoder.layers.0.norm3.bias', 'decoder.layers.4.norm3.bias', 'decoder.layers.2.multihead_attn.out_proj.weight', 'encoder.encoder.layer.11.attention.output.dense.weight', 'encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'decoder.layers.4.multihead_attn.out_proj.bias', 'encoder.encoder.layer.11.output.dense.weight', 'encoder.encoder.layer.6.intermediate.dense.bias', 'decoder.layers.7.self_attn.in_proj_bias', 'decoder.layers.8.multihead_attn.out_proj.bias', 'decoder.layers.1.norm1.weight', 'decoder.layers.6.norm3.weight', 'encoder.encoder.layer.4.output.dense.weight', 'encoder.encoder.layer.10.attention.self.value.weight', 'encoder.encoder.layer.6.output.dense.bias', 'decoder.layers.9.linear1.bias', 'encoder.encoder.layer.1.attention.self.value.bias', 'encoder.pooler.dense.bias', 'decoder.layers.0.norm3.weight', 'encoder.encoder.layer.6.attention.self.key.bias', 'encoder.encoder.layer.8.attention.self.query.bias', 'encoder.encoder.layer.9.attention.self.value.weight', 'decoder.layers.5.self_attn.in_proj_bias', 'encoder.encoder.layer.10.output.LayerNorm.bias', 'decoder.layers.9.norm1.bias', 'encoder.encoder.layer.3.intermediate.dense.bias', 'encoder.encoder.layer.1.intermediate.dense.bias', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.2.norm3.weight', 'encoder.encoder.layer.3.attention.self.query.weight', 'encoder.encoder.layer.8.intermediate.dense.weight', 'decoder.layers.5.norm3.bias', 'encoder.encoder.layer.5.attention.output.dense.bias', 'decoder.layers.6.self_attn.in_proj_weight', 'decoder.layers.7.linear1.weight', 'decoder.layers.3.linear1.bias', 'decoder.layers.5.self_attn.out_proj.weight', 'encoder.encoder.layer.4.attention.self.value.weight', 'decoder.layers.8.multihead_attn.in_proj_weight', 'encoder.encoder.layer.5.attention.output.dense.weight', 'decoder.layers.7.norm3.bias', 'decoder.layers.3.multihead_attn.in_proj_bias', 'decoder.layers.5.linear1.weight', 'decoder.layers.1.multihead_attn.out_proj.weight', 'encoder.encoder.layer.9.output.dense.bias', 'decoder.layers.5.linear2.weight', 'decoder.layers.6.self_attn.out_proj.bias', 'decoder.layers.9.self_attn.out_proj.weight', 'encoder.encoder.layer.5.output.LayerNorm.weight', 'decoder.layers.3.multihead_attn.in_proj_weight', 'decoder.layers.7.norm2.weight', 'encoder.encoder.layer.4.output.dense.bias', 'decoder.layers.5.multihead_attn.in_proj_weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.self.value.bias', 'decoder.layers.8.linear1.weight', 'encoder.encoder.layer.1.attention.self.key.weight', 'encoder.encoder.layer.0.attention.self.query.weight', 'encoder.encoder.layer.1.intermediate.dense.weight', 'decoder.layers.5.norm2.bias', 'decoder.layers.6.multihead_attn.in_proj_weight', 'decoder.layers.6.norm2.weight', 'decoder.layers.7.norm3.weight', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.2.norm2.weight', 'encoder.encoder.layer.8.attention.output.dense.bias', 'encoder.encoder.layer.2.attention.self.query.weight', 'encoder.encoder.layer.1.output.LayerNorm.weight', 'decoder.layers.5.self_attn.out_proj.bias', 'encoder.encoder.layer.7.attention.self.value.bias', 'encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.embeddings.position_ids', 'decoder.layers.1.norm1.bias', 'decoder.layers.4.norm3.weight', 'encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'decoder.layers.1.linear2.bias', 'encoder.encoder.layer.5.output.dense.bias', 'decoder.layers.6.norm1.weight', 'decoder.layers.7.linear2.weight', 'decoder.layers.3.linear1.weight', 'decoder.layers.1.multihead_attn.in_proj_bias', 'decoder.layers.7.linear1.bias', 'encoder.encoder.layer.11.attention.self.key.bias', 'decoder.layers.4.norm1.weight', 'encoder.encoder.layer.3.attention.output.dense.bias', 'bias', 'encoder.encoder.layer.7.attention.self.value.weight', 'encoder.encoder.layer.7.attention.self.key.bias', 'decoder.layers.8.norm3.bias', 'encoder.encoder.layer.10.intermediate.dense.bias', 'encoder.encoder.layer.9.output.LayerNorm.bias', 'encoder.encoder.layer.0.attention.output.dense.weight', 'decoder.layers.9.multihead_attn.out_proj.bias', 'encoder.encoder.layer.6.output.LayerNorm.weight', 'decoder.layers.9.norm2.bias', 'decoder.layers.6.multihead_attn.out_proj.bias', 'encoder.encoder.layer.7.output.dense.bias', 'encoder.encoder.layer.2.attention.output.dense.bias', 'decoder.layers.7.linear2.bias', 'encoder.encoder.layer.6.attention.self.value.weight', 'decoder.layers.0.self_attn.in_proj_weight', 'decoder.layers.3.self_attn.out_proj.bias', 'encoder.encoder.layer.9.output.dense.weight', 'encoder.encoder.layer.11.output.dense.bias', 'encoder.encoder.layer.7.output.LayerNorm.bias', 'encoder.encoder.layer.3.output.LayerNorm.bias', 'encoder.encoder.layer.6.attention.output.dense.bias', 'lm_head.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.7.multihead_attn.out_proj.bias', 'encoder.encoder.layer.4.attention.self.key.bias', 'encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.encoder.layer.4.attention.output.dense.weight', 'decoder.layers.6.multihead_attn.out_proj.weight', 'encoder.encoder.layer.3.attention.output.dense.weight', 'encoder.encoder.layer.6.output.dense.weight', 'dense.bias', 'decoder.layers.7.norm2.bias', 'encoder.encoder.layer.3.attention.self.key.bias', 'decoder.layers.7.multihead_attn.in_proj_bias', 'decoder.layers.3.linear2.weight', 'encoder.embeddings.token_type_embeddings.weight', 'encoder.encoder.layer.8.intermediate.dense.bias', 'decoder.layers.3.multihead_attn.out_proj.weight', 'decoder.layers.8.norm2.bias', 'decoder.layers.0.self_attn.out_proj.bias', 'encoder.encoder.layer.2.attention.self.value.weight', 'encoder.encoder.layer.11.attention.self.query.weight', 'encoder.encoder.layer.5.attention.self.query.bias', 'decoder.layers.8.linear2.bias', 'encoder.encoder.layer.9.intermediate.dense.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.4.multihead_attn.out_proj.weight', 'encoder.encoder.layer.11.attention.output.dense.bias', 'decoder.layers.8.linear1.bias', 'encoder.encoder.layer.7.attention.self.query.bias', 'encoder.encoder.layer.8.output.dense.weight', 'decoder.layers.2.norm1.weight', 'encoder.encoder.layer.0.output.dense.weight', 'decoder.layers.6.linear2.bias', 'encoder.encoder.layer.11.attention.self.query.bias', 'decoder.layers.0.norm1.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.8.self_attn.in_proj_bias', 'encoder.encoder.layer.6.attention.self.value.bias', 'decoder.layers.0.self_attn.in_proj_bias', 'encoder.encoder.layer.1.attention.self.query.weight', 'decoder.layers.9.linear2.bias', 'encoder.encoder.layer.3.attention.self.value.bias', 'encoder.encoder.layer.0.intermediate.dense.weight', 'encoder.encoder.layer.2.intermediate.dense.bias', 'encoder.encoder.layer.3.output.LayerNorm.weight', 'encoder.encoder.layer.10.intermediate.dense.weight', 'encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'decoder.layers.9.multihead_attn.in_proj_weight', 'decoder.layers.4.norm1.bias', 'encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.encoder.layer.6.intermediate.dense.weight', 'decoder.layers.9.linear2.weight', 'encoder.encoder.layer.11.attention.self.value.weight', 'encoder.encoder.layer.4.attention.output.dense.bias', 'decoder.layers.3.multihead_attn.out_proj.bias', 'decoder.layers.5.self_attn.in_proj_weight', 'encoder.encoder.layer.2.intermediate.dense.weight', 'encoder.encoder.layer.9.attention.self.query.weight', 'decoder.layers.0.multihead_attn.out_proj.weight', 'decoder.layers.9.linear1.weight', 'encoder.encoder.layer.4.attention.self.value.bias', 'decoder.layers.1.self_attn.in_proj_bias', 'encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'decoder.layers.8.norm2.weight', 'encoder.encoder.layer.10.attention.self.query.bias', 'encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.encoder.layer.4.attention.self.query.weight', 'decoder.layers.3.self_attn.in_proj_bias', 'encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'decoder.layers.1.self_attn.out_proj.bias', 'encoder.encoder.layer.9.intermediate.dense.weight', 'encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'decoder.layers.1.multihead_attn.in_proj_weight', 'decoder.layers.8.norm1.weight', 'decoder.layers.1.linear1.bias', 'decoder.layers.2.linear1.weight', 'encoder.encoder.layer.11.intermediate.dense.bias', 'decoder.layers.9.self_attn.in_proj_weight', 'encoder.encoder.layer.8.attention.self.query.weight', 'encoder.encoder.layer.6.attention.self.key.weight', 'decoder.layers.4.self_attn.in_proj_bias', 'encoder.encoder.layer.8.attention.self.value.bias', 'decoder.layers.0.linear1.bias', 'encoder.encoder.layer.4.attention.self.query.bias', 'decoder.layers.0.linear2.bias', 'encoder.encoder.layer.7.output.dense.weight', 'decoder.layers.2.self_attn.out_proj.bias', 'encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'decoder.layers.8.norm3.weight', 'encoder.encoder.layer.4.intermediate.dense.weight', 'encoder.encoder.layer.10.attention.output.dense.bias', 'decoder.layers.2.self_attn.in_proj_bias', 'encoder.encoder.layer.2.attention.self.key.weight', 'dense.weight', 'decoder.layers.9.self_attn.out_proj.bias', 'decoder.layers.8.multihead_attn.in_proj_bias', 'encoder.embeddings.LayerNorm.bias', 'encoder.encoder.layer.8.attention.self.key.weight', 'encoder.encoder.layer.3.attention.self.key.weight', 'encoder.encoder.layer.0.attention.self.value.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'decoder.layers.4.norm2.bias', 'decoder.layers.3.linear2.bias', 'decoder.layers.6.self_attn.in_proj_bias', 'encoder.encoder.layer.8.output.dense.bias', 'decoder.layers.4.linear1.weight', 'decoder.layers.7.self_attn.in_proj_weight', 'encoder.encoder.layer.3.output.dense.bias', 'encoder.encoder.layer.9.attention.output.dense.bias', 'decoder.layers.6.multihead_attn.in_proj_bias', 'encoder.encoder.layer.6.attention.self.query.weight', 'encoder.encoder.layer.4.output.LayerNorm.bias', 'encoder.encoder.layer.10.attention.self.key.bias', 'decoder.layers.9.norm3.weight', 'decoder.layers.3.norm1.bias', 'encoder.encoder.layer.5.attention.self.value.bias', 'encoder.encoder.layer.9.attention.self.value.bias', 'decoder.layers.1.norm2.bias', 'decoder.layers.2.multihead_attn.in_proj_bias', 'encoder.encoder.layer.2.output.dense.bias', 'decoder.layers.4.self_attn.in_proj_weight', 'encoder.encoder.layer.8.attention.output.dense.weight', 'decoder.layers.0.multihead_attn.in_proj_weight', 'encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.pooler.dense.weight', 'decoder.layers.2.multihead_attn.in_proj_weight', 'encoder.encoder.layer.1.output.LayerNorm.bias', 'encoder.encoder.layer.1.attention.self.value.weight', 'encoder.embeddings.position_embeddings.weight', 'encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.encoder.layer.7.attention.self.key.weight', 'decoder.layers.8.self_attn.out_proj.weight', 'decoder.layers.6.norm3.bias', 'encoder.encoder.layer.8.output.LayerNorm.weight', 'encoder.encoder.layer.0.output.LayerNorm.weight', 'decoder.layers.4.linear2.weight', 'decoder.layers.9.norm3.bias', 'encoder.encoder.layer.7.intermediate.dense.bias', 'decoder.layers.7.multihead_attn.in_proj_weight', 'encoder.encoder.layer.7.attention.self.query.weight', 'encoder.encoder.layer.7.intermediate.dense.weight', 'decoder.layers.5.norm1.weight', 'encoder.encoder.layer.5.output.dense.weight', 'decoder.layers.8.self_attn.in_proj_weight', 'decoder.layers.2.linear1.bias', 'encoder.encoder.layer.0.attention.output.dense.bias', 'decoder.layers.6.self_attn.out_proj.weight', 'encoder.encoder.layer.5.attention.self.key.bias', 'decoder.layers.3.norm3.weight', 'encoder.encoder.layer.3.attention.self.query.bias', 'encoder.encoder.layer.1.output.dense.bias', 'encoder.encoder.layer.4.attention.self.key.weight', 'encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.encoder.layer.9.attention.self.key.weight', 'decoder.layers.4.norm2.weight', 'decoder.layers.4.multihead_attn.in_proj_bias', 'encoder.encoder.layer.5.intermediate.dense.bias', 'encoder.embeddings.LayerNorm.weight', 'encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'decoder.layers.0.linear1.weight', 'encoder.encoder.layer.10.output.dense.weight', 'decoder.layers.8.self_attn.out_proj.bias', 'encoder.encoder.layer.11.output.LayerNorm.bias', 'decoder.layers.7.norm1.weight', 'encoder.encoder.layer.10.attention.self.query.weight', 'encoder.encoder.layer.9.attention.self.query.bias', 'encoder.encoder.layer.5.attention.self.query.weight', 'decoder.layers.5.multihead_attn.out_proj.bias', 'decoder.layers.5.norm2.weight', 'decoder.layers.8.linear2.weight', 'decoder.layers.2.self_attn.in_proj_weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at saved_models/dec_10_CombinedData_graphcodebert/methods-tests/checkpoint-best-bleu/pytorch_model.bin and are newly initialized: ['encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'pooler.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'pooler.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.4.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
01/12/2022 03:12:43 - INFO - __main__ -   reload model from saved_models/dec_10_CombinedData_graphcodebert/methods-tests/checkpoint-best-bleu/pytorch_model.bin
01/12/2022 03:12:47 - INFO - __main__ -   Test file: dataset/evosuit/Evosuit_test_lang3_ArrayUtils.methods,dataset/evosuit/Evosuit_test_lang3_ArrayUtils.tests
  0%|          | 0/682 [00:00<?, ?it/s]  1%|▏         | 10/682 [00:00<00:06, 97.51it/s]  4%|▍         | 26/682 [00:00<00:05, 127.59it/s]  6%|▌         | 42/682 [00:00<00:04, 138.49it/s]  8%|▊         | 56/682 [00:00<00:05, 122.76it/s] 10%|█         | 70/682 [00:00<00:04, 126.58it/s] 13%|█▎        | 87/682 [00:00<00:04, 139.89it/s] 15%|█▌        | 105/682 [00:00<00:03, 146.41it/s] 18%|█▊        | 120/682 [00:00<00:04, 124.22it/s] 20%|█▉        | 133/682 [00:01<00:04, 114.25it/s] 21%|██▏       | 145/682 [00:01<00:05, 105.23it/s] 24%|██▍       | 166/682 [00:01<00:03, 131.17it/s] 27%|██▋       | 184/682 [00:01<00:03, 141.92it/s] 30%|██▉       | 202/682 [00:01<00:03, 151.19it/s] 32%|███▏      | 220/682 [00:01<00:02, 159.06it/s] 35%|███▍      | 237/682 [00:01<00:03, 136.70it/s] 37%|███▋      | 252/682 [00:01<00:03, 122.04it/s] 39%|███▉      | 266/682 [00:02<00:03, 119.66it/s] 41%|████      | 279/682 [00:02<00:03, 111.08it/s] 43%|████▎     | 291/682 [00:02<00:03, 106.21it/s] 46%|████▌     | 314/682 [00:02<00:02, 136.45it/s] 48%|████▊     | 329/682 [00:02<00:02, 121.86it/s] 50%|█████     | 343/682 [00:02<00:02, 118.66it/s] 52%|█████▏    | 356/682 [00:02<00:02, 120.74it/s] 54%|█████▍    | 369/682 [00:03<00:03, 88.25it/s]  56%|█████▌    | 382/682 [00:03<00:03, 95.94it/s] 58%|█████▊    | 395/682 [00:03<00:02, 103.21it/s] 60%|██████    | 412/682 [00:03<00:02, 117.51it/s] 63%|██████▎   | 427/682 [00:03<00:02, 125.14it/s] 65%|██████▌   | 444/682 [00:03<00:01, 136.06it/s] 68%|██████▊   | 461/682 [00:03<00:01, 144.88it/s] 70%|██████▉   | 477/682 [00:03<00:01, 147.65it/s] 73%|███████▎  | 495/682 [00:03<00:01, 153.89it/s] 75%|███████▍  | 511/682 [00:04<00:01, 155.34it/s] 77%|███████▋  | 527/682 [00:04<00:01, 152.20it/s] 80%|███████▉  | 543/682 [00:04<00:00, 144.87it/s] 84%|████████▍ | 574/682 [00:04<00:00, 188.78it/s] 87%|████████▋ | 594/682 [00:04<00:00, 184.03it/s] 90%|████████▉ | 613/682 [00:04<00:00, 149.74it/s] 92%|█████████▏| 630/682 [00:04<00:00, 140.77it/s] 95%|█████████▍| 645/682 [00:04<00:00, 131.58it/s] 97%|█████████▋| 659/682 [00:05<00:00, 125.24it/s] 99%|█████████▊| 672/682 [00:05<00:00, 125.08it/s]100%|██████████| 682/682 [00:05<00:00, 129.84it/s]
/project/6025819/sepehr8/test_generation/GraphCodeBERT_TestGen/Env/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
  0%|          | 0/11 [00:00<?, ?it/s]/project/6025819/sepehr8/test_generation/GraphCodeBERT_TestGen/model.py:175: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  prevK = bestScoresId // numWords
  9%|▉         | 1/11 [02:21<23:36, 141.62s/it] 18%|█▊        | 2/11 [04:57<22:31, 150.14s/it] 27%|██▋       | 3/11 [07:36<20:33, 154.17s/it] 36%|███▋      | 4/11 [09:40<16:34, 142.07s/it] 45%|████▌     | 5/11 [12:18<14:46, 147.79s/it] 55%|█████▍    | 6/11 [14:25<11:44, 140.96s/it] 64%|██████▎   | 7/11 [16:13<08:40, 130.19s/it] 73%|███████▎  | 8/11 [17:43<05:51, 117.27s/it] 82%|████████▏ | 9/11 [19:16<03:39, 109.79s/it] 91%|█████████ | 10/11 [21:44<02:01, 121.64s/it]100%|██████████| 11/11 [23:39<00:00, 119.40s/it]run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
100%|██████████| 11/11 [23:39<00:00, 129.04s/it]
01/12/2022 03:36:34 - INFO - __main__ -     bleu-4 = 45.38 
01/12/2022 03:36:34 - INFO - __main__ -     xMatch = 3.5191 
01/12/2022 03:36:34 - INFO - __main__ -     ********************
01/12/2022 03:36:34 - INFO - __main__ -   Test file: dataset/evosuit/Evosuit_test_lang3_ArrayUtils.methods,dataset/evosuit/Evosuit_test_lang3_ArrayUtils.tests
  0%|          | 0/682 [00:00<?, ?it/s]  0%|          | 1/682 [00:00<02:00,  5.65it/s]  2%|▏         | 16/682 [00:00<00:09, 69.50it/s]  5%|▍         | 31/682 [00:00<00:06, 100.13it/s]  7%|▋         | 45/682 [00:00<00:05, 110.36it/s]  9%|▊         | 58/682 [00:00<00:05, 111.82it/s] 11%|█         | 73/682 [00:00<00:04, 122.41it/s] 13%|█▎        | 90/682 [00:00<00:04, 134.77it/s] 16%|█▌        | 106/682 [00:00<00:04, 141.46it/s] 18%|█▊        | 121/682 [00:01<00:04, 120.56it/s] 20%|█▉        | 134/682 [00:01<00:04, 110.68it/s] 21%|██▏       | 146/682 [00:01<00:05, 102.70it/s] 25%|██▍       | 170/682 [00:01<00:03, 136.42it/s] 27%|██▋       | 186/682 [00:01<00:03, 140.76it/s] 30%|███       | 206/682 [00:01<00:03, 156.27it/s] 33%|███▎      | 223/682 [00:01<00:02, 156.55it/s] 35%|███▌      | 240/682 [00:01<00:03, 131.66it/s] 37%|███▋      | 255/682 [00:02<00:05, 84.77it/s]  39%|███▉      | 267/682 [00:02<00:04, 90.18it/s] 41%|████      | 279/682 [00:02<00:04, 91.02it/s] 43%|████▎     | 290/682 [00:02<00:04, 92.25it/s] 46%|████▌     | 313/682 [00:02<00:03, 120.58it/s] 48%|████▊     | 327/682 [00:02<00:03, 114.95it/s] 50%|████▉     | 340/682 [00:03<00:03, 111.15it/s] 52%|█████▏    | 353/682 [00:03<00:02, 115.18it/s] 54%|█████▎    | 366/682 [00:03<00:02, 117.02it/s] 56%|█████▌    | 380/682 [00:03<00:02, 121.54it/s] 58%|█████▊    | 393/682 [00:03<00:02, 123.16it/s] 60%|█████▉    | 407/682 [00:03<00:02, 127.64it/s] 62%|██████▏   | 423/682 [00:03<00:01, 135.86it/s] 65%|██████▍   | 440/682 [00:03<00:01, 145.54it/s] 67%|██████▋   | 457/682 [00:03<00:01, 151.50it/s] 69%|██████▉   | 473/682 [00:03<00:01, 151.69it/s] 72%|███████▏  | 491/682 [00:04<00:01, 157.69it/s] 74%|███████▍  | 507/682 [00:04<00:01, 157.22it/s] 77%|███████▋  | 523/682 [00:04<00:01, 157.38it/s] 79%|███████▉  | 539/682 [00:04<00:00, 145.89it/s] 83%|████████▎ | 565/682 [00:04<00:00, 177.50it/s] 86%|████████▌ | 588/682 [00:04<00:00, 183.04it/s] 89%|████████▉ | 607/682 [00:04<00:00, 159.88it/s] 91%|█████████▏| 624/682 [00:04<00:00, 139.08it/s] 94%|█████████▎| 639/682 [00:05<00:00, 131.17it/s] 96%|█████████▌| 653/682 [00:05<00:00, 128.92it/s] 98%|█████████▊| 667/682 [00:05<00:00, 125.66it/s]100%|█████████▉| 680/682 [00:05<00:00, 122.77it/s]100%|██████████| 682/682 [00:05<00:00, 125.43it/s]
  0%|          | 0/11 [00:00<?, ?it/s]  9%|▉         | 1/11 [02:13<22:13, 133.40s/it] 18%|█▊        | 2/11 [04:48<21:57, 146.35s/it] 27%|██▋       | 3/11 [07:27<20:17, 152.13s/it] 36%|███▋      | 4/11 [09:30<16:22, 140.40s/it] 45%|████▌     | 5/11 [12:07<14:39, 146.66s/it] 55%|█████▍    | 6/11 [14:13<11:37, 139.45s/it] 64%|██████▎   | 7/11 [16:00<08:35, 128.99s/it] 73%|███████▎  | 8/11 [17:29<05:48, 116.20s/it] 82%|████████▏ | 9/11 [19:02<03:37, 108.85s/it] 91%|█████████ | 10/11 [21:29<02:00, 120.63s/it]100%|██████████| 11/11 [23:23<00:00, 118.70s/it]run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
run.py:269: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  attn_mask=np.zeros((self.args.max_source_length,self.args.max_source_length),dtype=np.bool)
100%|██████████| 11/11 [23:23<00:00, 127.63s/it]
01/12/2022 04:00:03 - INFO - __main__ -     bleu-4 = 45.38 
01/12/2022 04:00:03 - INFO - __main__ -     xMatch = 3.5191 
01/12/2022 04:00:03 - INFO - __main__ -     ********************
